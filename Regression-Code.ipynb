{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc3cfd70",
   "metadata": {
    "id": "cc3cfd70"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import traceback\n",
    "from pdb import set_trace\n",
    "import sys\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1de4402",
   "metadata": {
    "id": "e1de4402"
   },
   "outputs": [],
   "source": [
    "from util.timer import Timer\n",
    "from util.data import split_data, feature_label_split, Standardization\n",
    "from util.metrics import mse\n",
    "from datasets.HousingDataset import HousingDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MvWjqKVoHXts",
   "metadata": {
    "id": "MvWjqKVoHXts"
   },
   "source": [
    "Your Name: Type your name here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab441f67",
   "metadata": {
    "id": "ab441f67"
   },
   "source": [
    "# Understand Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d66c2",
   "metadata": {
    "id": "400d66c2"
   },
   "source": [
    "![](https://assets.prevu.com/blogs/images/first-time-buyer-boston-real-estate/03d0c13cdf6721a022afd91e343493b5?ixlib=rb-4.0.3&w=670&lossless=true&auto=format%20compress&fit=fill&fill=solid&s=cb885d7fc811865d8d2219c47c87eb01)\n",
    "\n",
    "The dataset you'll be using for this project is the Boston Housing dataset which contains various different features about houses in Boston. This is a classic machine learning dataset from 1978 and is one of the first datasets most people use when first learning machine learning. **There are 506 samples and 13 feature variables in this dataset. The objective is to predict the value of the house, given by the 'MEDV' column, using the provided features.**\n",
    "\n",
    "The dataset consists of 3 splits:\n",
    "\n",
    "1. **Train**: Throughout this assignment you will be training your model using this data.\n",
    "2. **Validation**: You will then use this set to tune your model and evaluate its performance.\n",
    "3. **Test**: This split simulates real life data which we often don't have access to until the model is deployed. We have kept this split hidden from you and we will use it to judge the performance of your model.\n",
    "\n",
    "You DO NOT have access to the Test set as it gonna be used for scoring. This will not prevent you to complete this assignment at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595eb378",
   "metadata": {
    "id": "595eb378"
   },
   "source": [
    "The meanings of features and target are listed below. However, the meanings of these attributes will not affect your coding. So read them only if you are interested. \n",
    "\n",
    "    1. CRIM      per capita crime rate by town\n",
    "    2. ZN        proportion of residential land zoned for lots over \n",
    "                 25,000 sq.ft.\n",
    "    3. INDUS     proportion of non-retail business acres per town\n",
    "    4. CHAS      Charles River dummy variable (= 1 if tract bounds \n",
    "                 river; 0 otherwise)\n",
    "    5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "    6. RM        average number of rooms per dwelling\n",
    "    7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "    8. DIS       weighted distances to five Boston employment centres\n",
    "    9. RAD       index of accessibility to radial highways\n",
    "    10. TAX      full-value property-tax rate per 10,000 USD\n",
    "    11. PTRATIO  pupil-teacher ratio by town\n",
    "    12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks \n",
    "                 by town\n",
    "    13. LSTAT    % lower status of the population\n",
    "    14. MEDV     Median value of owner-occupied homes (in 1000 USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c2d05",
   "metadata": {
    "id": "046c2d05"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d55f7b9",
   "metadata": {
    "id": "6d55f7b9"
   },
   "source": [
    "# Design Machine Learning Models (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f1ec3",
   "metadata": {
    "id": "912f1ec3"
   },
   "source": [
    "## Base Model\n",
    "Basic model structure, **don't change** this component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18e4417",
   "metadata": {
    "id": "d18e4417"
   },
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    \"\"\" Super class for ITCS Machine Learning Class\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5e9d1",
   "metadata": {
    "id": "ada5e9d1"
   },
   "source": [
    "## Linear Regression\n",
    "Basic model structure, **don't change** this component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df00cf0f",
   "metadata": {
    "id": "df00cf0f"
   },
   "outputs": [],
   "source": [
    "class LinearModel(BaseModel):\n",
    "    \"\"\"\n",
    "        Abstract class for a linear model\n",
    "\n",
    "        Attributes\n",
    "        ==========\n",
    "        w       ndarray\n",
    "                weight vector/matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            weight vector w is initialized as None\n",
    "        \"\"\"\n",
    "        self.w = None\n",
    "\n",
    "    # check if the matrix is 2-dimensional. if not, raise an exception\n",
    "    def _check_matrix(self, mat, name):\n",
    "        if len(mat.shape) != 2:\n",
    "            raise ValueError(f\"Your matrix {name} shape is not 2D! Matrix {name} has the shape {mat.shape}\")\n",
    "\n",
    "    # add a biases\n",
    "    def add_ones(self, X):\n",
    "        \"\"\"\n",
    "            add a column basis to X input matrix\n",
    "        \"\"\"\n",
    "        self._check_matrix(X, 'X')\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    ####################################################\n",
    "    #### abstract funcitons ############################\n",
    "    @abstractmethod\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "            train linear model\n",
    "\n",
    "            Args:\n",
    "                X:  Input data\n",
    "\n",
    "                y:  targets/labels\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "            apply the learned model to input X\n",
    "\n",
    "            parameters\n",
    "            ----------\n",
    "            X     2d array\n",
    "                  input data\n",
    "\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4babb8",
   "metadata": {
    "id": "7c4babb8"
   },
   "source": [
    "## TODO: Linear Regression with Ordinary Least Square (OLS) \n",
    "**Please complete the TODOs. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92df3a11",
   "metadata": {
    "id": "92df3a11"
   },
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquares(LinearModel):\n",
    "    \"\"\"\n",
    "        Performs regression using ordinary least squares\n",
    "        attributes:\n",
    "            w (np.ndarray): weight matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \n",
    "        \"\"\" Used to train our model to learn optimal weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to perform OLS in order to learn the\n",
    "                weights `self.w`.\n",
    "        \"\"\"\n",
    "        X_with_bias = self.add_ones(X)\n",
    "        X_transpose = X_with_bias.T\n",
    "        self.w = np.linalg.inv(X_transpose @ X_with_bias) @ X_transpose @ y\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Used to make a prediction using the learned weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to make a prediction given the learned\n",
    "                weights `self.w`.\n",
    "        \"\"\"\n",
    "        # TODO (REQUIRED) Add code below\n",
    "        X_with_bias = self.add_ones(X)\n",
    "        y_hat = X_with_bias @ self.w\n",
    "\n",
    "        # TODO (REQUIRED) Store predictions below by replacing np.ones()\n",
    "        #y_hat = np.ones([len(X), 1])\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e361f50",
   "metadata": {
    "id": "8e361f50"
   },
   "source": [
    "## TODO: Lienar Regression with least Mean Squares (LMS)\n",
    "Optimize the model through gradient descent. **Please complete the TODOs. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3becc614",
   "metadata": {
    "id": "3becc614"
   },
   "outputs": [],
   "source": [
    "class LeastMeanSquares(LinearModel):\n",
    "    \"\"\"\n",
    "        Performs regression using least mean squares (gradient descent)\n",
    "\n",
    "        attributes:\n",
    "            w (np.ndarray): weight matrix\n",
    "\n",
    "            alpha (float): learning rate or step size\n",
    "\n",
    "            epochs (int): Number of epochs to run for mini-batch\n",
    "                gradient descent\n",
    "\n",
    "            seed (int): Seed to be used for NumPy's RandomState class\n",
    "                or universal seed np.random.seed() function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float, epochs: int, seed: int = None):\n",
    "        super().__init__()\n",
    "        self.w = None\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\" Used to train our model to learn optimal weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to perform LMS in order to learn the\n",
    "                weights `self.w`.\n",
    "        \"\"\"\n",
    "        X_with_bias = self.add_ones(X)\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        self.w = np.random.randn(X_with_bias.shape[1], 1)\n",
    "        for _ in range(self.epochs):\n",
    "            y_pred = X_with_bias @ self.w\n",
    "            error = y_pred - y\n",
    "            gradient = (1/X_with_bias.shape[0]) * (X_with_bias.T @ error)\n",
    "            self.w = self.w - self.alpha * gradient\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Used to make a prediction using the learned weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to make a prediction given the learned\n",
    "                weights `self.w`.\n",
    "        \"\"\"\n",
    "        # TODO (REQUIRED) Add code below\n",
    "        X_with_bias = self.add_ones(X)\n",
    "        y_hat = X_with_bias @ self.w\n",
    "\n",
    "        # TODO (REQUIRED) Store predictions below by replacing np.ones()\n",
    "        # y_hat = np.ones([len(X), 1])\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273151d9",
   "metadata": {
    "id": "273151d9"
   },
   "source": [
    "## TODO: Polynomial Regression with Ordinary Least Square (OLS) \n",
    "**Please complete the TODOs. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "962ee175",
   "metadata": {
    "id": "962ee175"
   },
   "outputs": [],
   "source": [
    "class PolynomialRegression(OrdinaryLeastSquares):\n",
    "    \"\"\"\n",
    "        Performs polynomial regression using ordinary least squares algorithm\n",
    "\n",
    "        attributes:\n",
    "            w (np.ndarray): weight matrix that is inherited from OrdinaryLeastSquares\n",
    "\n",
    "            degree (int): the number of polynomial degrees to include when adding\n",
    "                polynomial features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degree: int):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "\n",
    "    def add_polynomial_features(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Computes polynomial features given the pass data.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to compute the polynomial features\n",
    "                for X. Be sure to return the new data with the polynomial features!\n",
    "\n",
    "            Hint:\n",
    "                Feel free to use sklearn.preprocessing.PolynomialFeatures but remember\n",
    "                it includes the bias so make sure to disable said feature!\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        poly = PolynomialFeatures(degree=self.degree, include_bias=False)\n",
    "        X_poly = poly.fit_transform(X)\n",
    "        \n",
    "        return X_poly\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\" Used to train our model to learn optimal weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to perform polynomial regression using\n",
    "                the closed form solution OLS to learn the weights `self.w`.\n",
    "\n",
    "            Hint:\n",
    "                Since we inherit from OrdinaryLeastSquares you can simply just call\n",
    "                super().train(X, y) instead of copying the code from OrdinaryLeastSquares\n",
    "                after you run self.add_polynomial_features(X).\n",
    "        \"\"\"\n",
    "        X_poly = self.add_polynomial_features(X)\n",
    "        super().fit(X_poly, y)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Used to make a prediction using the learned weights.\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to make a prediction given the learned\n",
    "                weights `self.w`.\n",
    "\n",
    "            Hint:\n",
    "                Since we inherit from OrdinaryLeastSquares you can simply just call\n",
    "                super().predict(X) instead of copying the code from OrdinaryLeastSquares\n",
    "                after you run self.add_polynomial_features(X).\n",
    "        \"\"\"\n",
    "        # TODO (REQUIRED) Add code below\n",
    "        X_poly = self.add_polynomial_features(X)\n",
    "        y_hat = super().predict(X_poly)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d18bb",
   "metadata": {
    "id": "a05d18bb"
   },
   "source": [
    "## TODO: Polynomial Regression with OLS and Regularization\n",
    "**Please complete the TODOs. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7ff8548",
   "metadata": {
    "id": "d7ff8548"
   },
   "outputs": [],
   "source": [
    "class PolynomialRegressionRegularized(PolynomialRegression):\n",
    "    \"\"\"\n",
    "        Performs polynomial regression with l2 regularization using the ordinary least squares algorithm\n",
    "    \n",
    "        attributes:\n",
    "            w (np.ndarray): weight matrix that is inherited from OrdinaryLeastSquares\n",
    "            \n",
    "            degree (int): the number of polynomial degrees to include when adding\n",
    "                polynomial features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degree: int, lamb: float):\n",
    "        super().__init__(degree)\n",
    "        self.lamb = lamb\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\" Used to train our model to learn optimal weights.\n",
    "        \n",
    "            TODO:\n",
    "                Finish this method by adding code to perform polynomial regression using\n",
    "                the closed form solution OLS with L2 regularization to learn \n",
    "                the weights `self.w`.\n",
    "                \n",
    "            Hint:\n",
    "                Add the bias after computing polynomial features. Typically we don't want\n",
    "                to include the bias when computing polynomial features.\n",
    "        \"\"\"\n",
    "        X_poly = self.add_polynomial_features(X)\n",
    "    \n",
    "        X_poly_with_bias = self.add_ones(X_poly)\n",
    "        n_features = X_poly_with_bias.shape[1]\n",
    "        reg_matrix = np.eye(n_features)\n",
    "        reg_matrix[0, 0] = 0  \n",
    "        \n",
    "        X_transpose = X_poly_with_bias.T\n",
    "        self.w = np.linalg.inv(X_transpose @ X_poly_with_bias + self.lamb * reg_matrix) @ X_transpose @ y\n",
    "\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Used to make a prediction using the learned weights.\n",
    "\n",
    "            TODO:\n",
    "                This predict() method is exactly the same as the predict() method in the above class `PolynomialRegression`, \n",
    "                so you can just simply copy them here. \n",
    "        \"\"\"\n",
    "        # TODO (REQUIRED) Add code below\n",
    "        X_poly = self.add_polynomial_features(X)\n",
    "        y_hat = super().predict(X_poly)\n",
    "        # TODO (REQUIRED) Store predictions below by replacing np.ones()\n",
    "        #y_hat = np.ones([len(X), 1])\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc97eb",
   "metadata": {
    "id": "08cc97eb"
   },
   "source": [
    "# TODO: Define Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7687cf22",
   "metadata": {
    "id": "7687cf22"
   },
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_params(name):\n",
    "        model = getattr(HyperParameters, name)\n",
    "        return {key:value for key, value in model.__dict__.items() \n",
    "            if not key.startswith('__') and not callable(key)}\n",
    "    \n",
    "    class OrdinaryLeastSquares():\n",
    "        pass # No hyperparamters to set\n",
    "        \n",
    "    class LeastMeanSquares():\n",
    "        model_kwargs = dict(\n",
    "            alpha = 0.01, # TODO (REQUIRED) Set your learning rate\n",
    "            epochs = 1000, # TODO (OPTIONAL) Set number of epochs\n",
    "            seed = 42, # TODO (OPTIONAL) Set seed for randomly generated weights\n",
    "        )\n",
    "\n",
    "        data_prep_kwargs = dict(\n",
    "            # TODO (OPTIONAL) Set the names of the features/columns to use for the Housing dataset\n",
    "            use_features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "                            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'],\n",
    "        )\n",
    "\n",
    "    class PolynomialRegression():\n",
    "        model_kwargs = dict(\n",
    "            degree = 3, # TODO (REQUIRED) Set your polynomial degree\n",
    "        )\n",
    "        \n",
    "    class PolynomialRegressionRegularized():\n",
    "        model_kwargs = dict(\n",
    "            degree = 3, # TODO (REQUIRED) Set your polynomial degree\n",
    "            lamb = 0.1, # TODO (REQUIRED) Set your regularization value for lambda\n",
    "        )\n",
    "\n",
    "        data_prep_kwargs = dict(\n",
    "            # TODO (OPTIONAL) Set the names of the features/columns to use for the Housing dataset\n",
    "            use_features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "                            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd692453",
   "metadata": {
    "id": "bd692453"
   },
   "source": [
    "# Model Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4711cf4",
   "metadata": {
    "id": "c4711cf4"
   },
   "source": [
    "## Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f878e70",
   "metadata": {
    "id": "1f878e70"
   },
   "outputs": [],
   "source": [
    "def standardize_data(X_trn, X_vld):\n",
    "    standardize = Standardization()\n",
    "    X_trn_clean = standardize.fit_transform(X_trn)\n",
    "    X_eval_clean = standardize.transform(X_vld)\n",
    "    \n",
    "    return X_trn_clean, X_eval_clean\n",
    "\n",
    "def get_cleaned_data(df_trn, df_vld, feature_names, label_name, return_df=False):\n",
    "    X_trn, y_trn, X_vld, y_vld = split_data(df_trn, df_vld, feature_names, label_name)\n",
    "    X_trn, X_vld = standardize_data(X_trn, X_vld)\n",
    "\n",
    "    return X_trn, y_trn, X_vld, y_vld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97c266ec",
   "metadata": {
    "id": "97c266ec"
   },
   "outputs": [],
   "source": [
    "task_info = [\n",
    "       dict(\n",
    "            model=OrdinaryLeastSquares,\n",
    "            name='OrdinaryLeastSquares',\n",
    "            threshold=80,\n",
    "            metrics=dict(MSE=mse),\n",
    "            eval_metric='MSE',\n",
    "            trn_score=9999,\n",
    "            eval_score=9999,\n",
    "            successful=False,\n",
    "        ),\n",
    "        dict(\n",
    "            model=LeastMeanSquares,\n",
    "            name='LeastMeanSquares',\n",
    "            threshold=80,\n",
    "            metrics=dict(MSE=mse),\n",
    "            eval_metric='MSE',\n",
    "            trn_score=9999,\n",
    "            eval_score=9999,\n",
    "            successful=False,\n",
    "        ),\n",
    "        dict(\n",
    "            model=PolynomialRegression,\n",
    "            name='PolynomialRegression',\n",
    "            threshold=50,\n",
    "            metrics=dict(MSE=mse),\n",
    "            eval_metric='MSE',\n",
    "            trn_score=9999,\n",
    "            eval_score=9999,\n",
    "            successful=False,\n",
    "        ),\n",
    "        dict(\n",
    "            model=PolynomialRegressionRegularized,\n",
    "            name='PolynomialRegressionRegularized',\n",
    "            threshold=40,\n",
    "            metrics=dict(MSE=mse),\n",
    "            eval_metric='MSE',\n",
    "            trn_score=9999,\n",
    "            eval_score=9999,\n",
    "            successful=False,\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951dc98",
   "metadata": {
    "id": "c951dc98"
   },
   "source": [
    "## Define Model running (training/fit and testing/evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6edccff",
   "metadata": {
    "id": "b6edccff"
   },
   "outputs": [],
   "source": [
    "def get_name(obj):\n",
    "    try:\n",
    "        if hasattr(obj, '__name__'):\n",
    "            return obj.__name__\n",
    "        else:\n",
    "            return obj\n",
    "    except Exception as e:\n",
    "        return obj\n",
    "    \n",
    "def catch_and_throw(e, err):\n",
    "    trace = traceback.format_exc()\n",
    "    print(err + f\"\\n{trace}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "657ddf50",
   "metadata": {
    "id": "657ddf50"
   },
   "outputs": [],
   "source": [
    "class RunModel():\n",
    "    t1 = '\\t'\n",
    "    t2 = '\\t\\t'\n",
    "    t3 = '\\t\\t\\t'\n",
    "    def __init__(self, model, model_params):\n",
    "        self.model_name = model.__name__\n",
    "        self.model_params = model_params\n",
    "        self.model = self.build_model(model, model_params)\n",
    "\n",
    "    def build_model(self, model, model_params):\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Building model {self.model_name}\")\n",
    "        \n",
    "        try:\n",
    "            model = model(**model_params)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while building model for {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        print(f\"Training {self.model_name}...\")\n",
    "        print(f\"{self.t1}Using hyperparameters: \")\n",
    "        [print(f\"{self.t2}{n} = {get_name(v)}\")for n, v in self.model_params.items()]\n",
    "        try: \n",
    "            return self._fit(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while training model for {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "            \n",
    "    def _fit(self, X, y, metrics=None, pass_y=False):\n",
    "        if pass_y:\n",
    "            self.model.fit(X, y)\n",
    "        else:\n",
    "             self.model.fit(X)\n",
    "        preds = self.model.predict(X)\n",
    "        scores = self.get_metrics(y, preds, metrics, prefix='Train')\n",
    "        return scores\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        print(f\"Evaluating {self.model_name}...\")\n",
    "        try:\n",
    "            return self._evaluate(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while evaluating model for {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "        \n",
    "\n",
    "    def _evaluate(self, X, y, metrics, prefix=''):\n",
    "        preds = self.model.predict(X)\n",
    "        scores = self.get_metrics(y, preds, metrics, prefix)      \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        try:\n",
    "            preds = self.model.predict(X)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while making predictions for model {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "            \n",
    "        return preds\n",
    "    \n",
    "    def get_metrics(self, y, y_hat, metrics, prefix=''):\n",
    "        scores = {}\n",
    "        for name, metric in metrics.items():\n",
    "            score = metric(y, y_hat)\n",
    "            display_score = round(score, 3)\n",
    "            scores[name] = score\n",
    "            print(f\"{self.t2}{prefix} {name}: {display_score}\")\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edb24c7b",
   "metadata": {
    "id": "edb24c7b"
   },
   "outputs": [],
   "source": [
    "def run_eval(eval_stage='validation',task_infos=task_info):\n",
    "    main_timer = Timer()\n",
    "    main_timer.start()\n",
    "\n",
    "    dataset = HousingDataset()\n",
    "    df_trn, df_vld = dataset.load()\n",
    "\n",
    "    total_points = 0\n",
    "      \n",
    "    for info in task_infos:\n",
    "        task_timer =  Timer()\n",
    "        task_timer.start()\n",
    "        try: \n",
    "            params = HyperParameters.get_params(info['name'])\n",
    "            model_kwargs = params.get('model_kwargs', {})\n",
    "            data_prep_kwargs = params.get('data_prep_kwargs', {})\n",
    "\n",
    "            if info['name'] == 'OrdinaryLeastSquares':\n",
    "                feature_names = \"RM\"\n",
    "            elif info['name'] == 'PolynomialRegression':\n",
    "                feature_names = \"LSTAT\" \n",
    "            else:\n",
    "                use_features = data_prep_kwargs.get('use_features')\n",
    "                if use_features is None:\n",
    "                    err = f\"use_features argument for {info['name']} can not be none: received {use_features}\"\n",
    "                    raise ValueError(err)\n",
    "                elif  len(use_features) < 2 :\n",
    "                    err = f\"use_features argument for {info['name']} must have at least 2 features: received {use_features}\"\n",
    "                    raise ValueError(err)\n",
    "                \n",
    "                feature_names = data_prep_kwargs['use_features']\n",
    "\n",
    "            run_model = RunModel(info['model'], model_kwargs)\n",
    "\n",
    "            \n",
    "            X_trn, y_trn, X_vld, y_vld = get_cleaned_data(df_trn, df_vld, feature_names, \"MEDV\")\n",
    "\n",
    "            trn_scores = run_model.fit(X_trn, y_trn, info['metrics'], pass_y=True)  # Model training\n",
    "            eval_scores = run_model.evaluate(X_vld, y_vld, info['metrics'], prefix=eval_stage.capitalize()) # Model testing\n",
    "\n",
    "            info['trn_score'] = trn_scores[info['eval_metric']]\n",
    "            info['eval_score'] = eval_scores[info['eval_metric']]\n",
    "            info['successful'] = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            track = traceback.format_exc()\n",
    "            print(\"The following exception occurred while executing this test case:\\n\", track)\n",
    "        task_timer.stop()\n",
    "        \n",
    "        print(\"\")\n",
    "        points = rubric_regression(info['eval_score'], info['threshold'])\n",
    "        print(f\"Points Earned: {points}\")\n",
    "        total_points += points\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print('')\n",
    "    main_timer.stop()\n",
    "\n",
    "    avg_trn_mse, avg_eval_mse, successful_tests = summary(task_info)\n",
    "    task_eval_mse = get_eval_scores(task_info)\n",
    "    total_points = int(round(total_points))\n",
    "\n",
    "    print(f\"MSE averages for {successful_tests} successful tests\")\n",
    "    print(f\"\\tAverage Train MSE: {avg_trn_mse}\")\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"\\tAverage {eval_stage.capitalize()} MSE: {avg_eval_mse}\")\n",
    "    \n",
    "    return (total_points, avg_trn_mse, avg_eval_mse, *task_eval_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a68e4",
   "metadata": {
    "id": "391a68e4"
   },
   "source": [
    "## Evaluation Related Functions\n",
    "Don't change this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d653ca94",
   "metadata": {
    "id": "d653ca94"
   },
   "outputs": [],
   "source": [
    "def rubric_regression(mse, thresh, max_score=20):\n",
    "    if mse <= thresh:\n",
    "        score_percent = 100\n",
    "    elif mse is not None:\n",
    "        score_percent = (thresh / mse) * 100\n",
    "        if score_percent < 40:\n",
    "            score_percent = 40\n",
    "    else:\n",
    "        score_percent = 20\n",
    "    score = max_score * score_percent / 100.0\n",
    "\n",
    "    return score\n",
    "\n",
    "def get_eval_scores(task_info):\n",
    "    return [i['eval_score'] for i in task_info]\n",
    "\n",
    "def summary(task_info):\n",
    "    sum_trn_mse = 0\n",
    "    sum_eval_mse = 0\n",
    "    successful_tests = 0\n",
    "\n",
    "    for info in task_info:\n",
    "        if info['successful']:\n",
    "            successful_tests += 1\n",
    "            sum_trn_mse += info['trn_score']\n",
    "            sum_eval_mse += info['eval_score']\n",
    "    \n",
    "    if successful_tests == 0:\n",
    "        return 9999, 9999, successful_tests\n",
    "    \n",
    "    avg_trn_mse = sum_trn_mse / successful_tests\n",
    "    avg_eval_mse = sum_eval_mse / successful_tests\n",
    "    return avg_trn_mse, avg_eval_mse, successful_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c0ef0",
   "metadata": {
    "id": "ad7c0ef0"
   },
   "source": [
    "# Test your code\n",
    "Run the following cell to test your code (or for **debugging**). Currently, the last line of output is \"Average Validation MSE: 548.01\". The number is super high because the TODOs are still empty (no machine learning model is used) right now. **Please fill in the TODOs through this ipython file and try your best to get a low MSE.** The \"Average Validation MSE\" will decide your score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58c6bd3b",
   "metadata": {
    "id": "58c6bd3b",
    "outputId": "f8437391-82f9-41be-92f7-13a4686d1082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping download. File already exists: /Users/hritz/Downloads/Regression-Assignment/datasets/data/housing.train\n",
      "\n",
      "Skipping download. File already exists: /Users/hritz/Downloads/Regression-Assignment/datasets/data/housing.val\n",
      "\n",
      "Skipping download. File already exists: /Users/hritz/Downloads/Regression-Assignment/datasets/data/housing.names\n",
      "\n",
      "==================================================\n",
      "Building model OrdinaryLeastSquares\n",
      "Training OrdinaryLeastSquares...\n",
      "\tUsing hyperparameters: \n",
      "\t\tTrain MSE: 41.941\n",
      "Evaluating OrdinaryLeastSquares...\n",
      "\t\tValidation MSE: 47.325\n",
      "Elapsed time: 0.0165 seconds\n",
      "\n",
      "Points Earned: 20.0\n",
      "==================================================\n",
      "Building model LeastMeanSquares\n",
      "Training LeastMeanSquares...\n",
      "\tUsing hyperparameters: \n",
      "\t\talpha = 0.01\n",
      "\t\tepochs = 1000\n",
      "\t\tseed = 42\n",
      "\t\tTrain MSE: 21.507\n",
      "Evaluating LeastMeanSquares...\n",
      "\t\tValidation MSE: 23.324\n",
      "Elapsed time: 0.0126 seconds\n",
      "\n",
      "Points Earned: 20.0\n",
      "==================================================\n",
      "Building model PolynomialRegression\n",
      "Training PolynomialRegression...\n",
      "\tUsing hyperparameters: \n",
      "\t\tdegree = 3\n",
      "\t\tTrain MSE: 31.826\n",
      "Evaluating PolynomialRegression...\n",
      "\t\tValidation MSE: 21.925\n",
      "Elapsed time: 0.0070 seconds\n",
      "\n",
      "Points Earned: 20.0\n",
      "==================================================\n",
      "Building model PolynomialRegressionRegularized\n",
      "Training PolynomialRegressionRegularized...\n",
      "\tUsing hyperparameters: \n",
      "\t\tdegree = 3\n",
      "\t\tlamb = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hritz/Downloads/Regression-Assignment/datasets/HousingDataset.py:79: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_train = pd.read_csv(self.data[\"paths\"][\"train\"], delim_whitespace=True,\n",
      "/Users/hritz/Downloads/Regression-Assignment/datasets/HousingDataset.py:82: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_val = pd.read_csv(self.data[\"paths\"][\"val\"], delim_whitespace=True,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     run_eval()\n",
      "Cell \u001b[0;32mIn[33], line 38\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(eval_stage, task_infos)\u001b[0m\n\u001b[1;32m     33\u001b[0m run_model \u001b[38;5;241m=\u001b[39m RunModel(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], model_kwargs)\n\u001b[1;32m     36\u001b[0m X_trn, y_trn, X_vld, y_vld \u001b[38;5;241m=\u001b[39m get_cleaned_data(df_trn, df_vld, feature_names, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMEDV\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m trn_scores \u001b[38;5;241m=\u001b[39m run_model\u001b[38;5;241m.\u001b[39mfit(X_trn, y_trn, info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m], pass_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[1;32m     39\u001b[0m eval_scores \u001b[38;5;241m=\u001b[39m run_model\u001b[38;5;241m.\u001b[39mevaluate(X_vld, y_vld, info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m], prefix\u001b[38;5;241m=\u001b[39meval_stage\u001b[38;5;241m.\u001b[39mcapitalize()) \u001b[38;5;66;03m# Model testing\u001b[39;00m\n\u001b[1;32m     41\u001b[0m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrn_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m trn_scores[info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[32], line 26\u001b[0m, in \u001b[0;36mRunModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m [\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_name(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;28;01mfor\u001b[39;00m n, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_params\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     28\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException caught while training model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[32], line 36\u001b[0m, in \u001b[0;36mRunModel._fit\u001b[0;34m(self, X, y, metrics, pass_y)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m---> 36\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m     37\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metrics(y, preds, metrics, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "Cell \u001b[0;32mIn[27], line 48\u001b[0m, in \u001b[0;36mPolynomialRegressionRegularized.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# TODO (REQUIRED) Add code below\u001b[39;00m\n\u001b[1;32m     47\u001b[0m X_poly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_polynomial_features(X)\n\u001b[0;32m---> 48\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(X_poly)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# TODO (REQUIRED) Store predictions below by replacing np.ones()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#y_hat = np.ones([len(X), 1])\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
      "Cell \u001b[0;32mIn[26], line 61\u001b[0m, in \u001b[0;36mPolynomialRegression.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Used to make a prediction using the learned weights.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    TODO:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m        after you run self.add_polynomial_features(X).\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# TODO (REQUIRED) Add code below\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m X_poly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_polynomial_features(X)\n\u001b[1;32m     62\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(X_poly)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
      "Cell \u001b[0;32mIn[26], line 29\u001b[0m, in \u001b[0;36mPolynomialRegression.add_polynomial_features\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolynomialFeatures\n\u001b[1;32m     28\u001b[0m poly \u001b[38;5;241m=\u001b[39m PolynomialFeatures(degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree, include_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m X_poly \u001b[38;5;241m=\u001b[39m poly\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_poly\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_polynomial.py:555\u001b[0m, in \u001b[0;36mPolynomialFeatures.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;66;03m# XP[:, start:end] are terms of degree d - 1\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# that exclude feature #feature_idx.\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m     np\u001b[38;5;241m.\u001b[39mmultiply(\n\u001b[1;32m    556\u001b[0m         XP[:, start:end],\n\u001b[1;32m    557\u001b[0m         X[:, feature_idx : feature_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    558\u001b[0m         out\u001b[38;5;241m=\u001b[39mXP[:, current_col:next_col],\n\u001b[1;32m    559\u001b[0m         casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    561\u001b[0m     current_col \u001b[38;5;241m=\u001b[39m next_col\n\u001b[1;32m    563\u001b[0m new_index\u001b[38;5;241m.\u001b[39mappend(current_col)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_eval()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e69b7",
   "metadata": {
    "id": "944e69b7"
   },
   "source": [
    "# Easy Points (20 points)\n",
    "In case the above coding is too difficult to you guys, here are simple questions which will counts for 20 points (the above coding counts for 80 points) of this assignment. Answer the questions based your understanding of the dataset. Each question worths 2 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a596435",
   "metadata": {
    "id": "5a596435"
   },
   "source": [
    "Q1: In training dataset, there are how many samples?\n",
    "\n",
    "A1: 323 \n",
    "\n",
    "code used- [ if __name__ == \"__main__\":\n",
    "    dataset = HousingDataset()\n",
    "    df_train, df_val = dataset.load()\n",
    "    print(f\"Number of samples in training dataset: {len(df_train)}\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3d932",
   "metadata": {
    "id": "bde3d932"
   },
   "source": [
    "Q2: In validation dataset, there are how many samples?\n",
    "\n",
    "A2: 81 \n",
    "\n",
    "code used- [if __name__ == \"__main__\":\n",
    "    dataset = HousingDataset()\n",
    "    df_train, df_val = dataset.load()\n",
    "    print(f\"Number of samples in validation dataset: {len(df_val)}\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d8cfc",
   "metadata": {
    "id": "f90d8cfc"
   },
   "source": [
    "Q3: In each sample, there are how many input features (or variable)?\n",
    "\n",
    "A3: Number of input features is 13 and 1 target variable in each sample.\n",
    "\n",
    "code used- [if __name__ == \"__main__\":\n",
    "    dataset = HousingDataset()\n",
    "    df_train, df_val = dataset.load()\n",
    "    input_features = len(dataset.data[\"columns\"]) - 1\n",
    "    print(f\"Number of input features: {input_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb7921",
   "metadata": {
    "id": "71fb7921"
   },
   "source": [
    "Q5: What are the input variable (or input features)? Please answer the column names of the input features. Don't include quote mark. \n",
    "\n",
    "A5: The input features are:\n",
    "CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT\n",
    "\n",
    "These are the 13 columns used as input features, while MEDV (median value of owner-occupied homes) is the target variable that the model would predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c7fe9",
   "metadata": {
    "id": "2c9c7fe9"
   },
   "source": [
    "Q6: In training datset, what's the mean value of `RM` (Keep two decimal places)? \n",
    "\n",
    "A6: RM in training dataset is 6.28\n",
    "\n",
    "code used- [if __name__ == \"__main__\":\n",
    "    dataset = HousingDataset()\n",
    "    df_train, df_val = dataset.load()\n",
    "    mean_rm = df_train['RM'].mean()\n",
    "    print(f\"Mean value of RM in training dataset: {mean_rm:.2f}\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6ae87",
   "metadata": {
    "id": "7ba6ae87"
   },
   "source": [
    "Q7: **After standarldization**, what's the value of `AGE` variable of the 1st sample (the 0-th sample) in training set (Keep two decimal places)? \n",
    "\n",
    "A7: Standardized AGE value for first sample: -0.76\n",
    "\n",
    "code used- [if __name__ == \"__main__\":\n",
    "    dataset = HousingDataset()\n",
    "    df_train, df_val = dataset.load()\n",
    "\n",
    "    age_mean = df_train['AGE'].mean()\n",
    "    age_std = df_train['AGE'].std()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56988b75",
   "metadata": {
    "id": "56988b75"
   },
   "source": [
    "Q8: The output/target variable is 'Continuous' or 'Discrete'?\n",
    "\n",
    "A8: Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a772df",
   "metadata": {
    "id": "46a772df"
   },
   "source": [
    "Q9: Will the regression with Ordinary Least Square based on gradient descent or not? Please answer YES or NO.\n",
    "\n",
    "A9: NO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a949279",
   "metadata": {
    "id": "0a949279"
   },
   "source": [
    "Q10: In Regularization, increase the coefficient lambda, the model will be more 'Simple' or 'Complex'? \n",
    "\n",
    "A10: Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac6bdbd",
   "metadata": {
    "id": "aac6bdbd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
