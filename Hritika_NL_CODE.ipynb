{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facef8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /opt/anaconda3/lib/python3.12/site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6c6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import traceback\n",
    "from pdb import set_trace\n",
    "import sys\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import traceback\n",
    "from typing import List, Tuple, Union, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e1521f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.timer import Timer\n",
    "from util.data import split_data, dataframe_to_array, binarize_classes\n",
    "from util.metrics import accuracy\n",
    "from util.metrics import mse\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from util.metrics import nll, sse\n",
    "from util.data import AddBias, Standardization, ImageNormalization, OneHotEncoding\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datasets.MNISTDataset import MNISTDataset\n",
    "from datasets.HousingDataset import HousingDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b226d",
   "metadata": {},
   "source": [
    "Your Name: HRITIKA KUCHERIYA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7bc48",
   "metadata": {},
   "source": [
    "# Understand the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f051b1f",
   "metadata": {},
   "source": [
    "In this programming assignment you will be implementing and training **two** neural networks. Each neural network will be trained on a different set of data. You will utilize the Boston House Pricing regression dataset and MNIST handwritten digit classification dataset. To account for the different ML problems and datasets you will need to implement the following neural networks:\n",
    "\n",
    "1. **Regressor neural network** which predicts the median value of owner-occupied homes in $1000's\n",
    "2. **Classifier neural network** for multi-class classification that predicts the digits 0-9 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb08ee3f",
   "metadata": {},
   "source": [
    "## Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e2359e",
   "metadata": {},
   "source": [
    "![](https://assets.prevu.com/blogs/images/first-time-buyer-boston-real-estate/03d0c13cdf6721a022afd91e343493b5?ixlib=rb-4.0.3&w=670&lossless=true&auto=format%20compress&fit=fill&fill=solid&s=cb885d7fc811865d8d2219c47c87eb01)\n",
    "\n",
    "The first dataset you'll be using for this project is the Boston Housing dataset which contains various different features about houses in Boston. This is a classic machine learning dataset from 1978 and is one of the first datasets most people use when first learning machine learning. **There are 506 samples and 13 feature variables in this dataset.**\n",
    "\n",
    "The dataset consists of 3 splits:\n",
    "\n",
    "1. **Train**: Throughout this assignment you will be training your model using this data.\n",
    "2. **Validation**: You will then use this set to tune your model and evaluate its performance.\n",
    "3. **Test**: This split simulates real life data which we often don't have access to until the model is deployed. We have kept this split hidden from you and we will use it to judge the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77836fdd",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/01c057a753e92a9bc70b8c45d62b295431851c09cffadf53106fc0aea7e2843f/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067)\n",
    "The second dataset you'll be using for this project is the famous [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset which contains images of handwritten digits 0 through 9. There are 60,000 images included in the dataset and each image is a gray scale image of size 28x28. Each pixel represents a feature which means there are $28*28$ or $784$ features per each data sample.\n",
    "\n",
    "The dataset consists of 3 splits:\n",
    "\n",
    "1. **Train**: Throughout this assignment you will be training your model using this data. There are approximately 44k training samples.\n",
    "2. **Validation**: You will then use this set to tune your model and evaluate its performance. There are approximately 12k training samples.\n",
    "3. **Test**: This split simulates real life data which we often don't have access to until the model is deployed. We have kept this split hidden from you and we will use it to judge the performance of your model on Autolab.\n",
    "\n",
    "You DO NOT have access to the Test set as it gonna be used for scoring. This will not prevent you to complete this assignment at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd2f66",
   "metadata": {},
   "source": [
    "# Design Machine Learning Models (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e9636",
   "metadata": {},
   "source": [
    "## Base Model\n",
    "Basic model structure, **don't change** this component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d0094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    \"\"\" Super class for ITCS Machine Learning Class\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accf6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\" Class which stores all variables required for a layer in a neural network\n",
    "    \n",
    "        Attributes:\n",
    "            W: NumPy array of weights for all neurons in the layer\n",
    "            \n",
    "            b: NumPy array of biases for all neurons in the layer\n",
    "            \n",
    "            g: Activation function for all neurons in the layer\n",
    "            \n",
    "            name: Name of the layer\n",
    "            \n",
    "            neurons: Number of neurons in the layer\n",
    "            \n",
    "            inputs: Number of inputs into the layer\n",
    "            \n",
    "            Z: Linear combination of weights and inputs for all neurons. \n",
    "                Initialized to an empty array until it is computed and set.\n",
    "                \n",
    "            A: Activation output for all neurons. Initialized to an empty \n",
    "                array until it is computed and set.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        W:np.array, \n",
    "        b:np.array, \n",
    "        g: object, \n",
    "        name: str=\"\"\n",
    "    ):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.g = g\n",
    "        self.name = name \n",
    "        self.neurons = len(W)\n",
    "        self.inputs = W.shape[1]\n",
    "        self.Z = np.array([])\n",
    "        self.A = np.array([])\n",
    "    \n",
    "    def print_info(self) -> None:\n",
    "        \"\"\" Prints info for all class attributes\"\"\"\n",
    "        print(f\"{self.name}\")\n",
    "        print(f\"\\tNeurons: {self.neurons}\")\n",
    "        print(f\"\\tInputs: {self.inputs}\")\n",
    "        print(f\"\\tWeight shape: {self.W.shape}\")\n",
    "        print(f\"\\tBias shape: {self.b.shape}\")\n",
    "        print(f\"\\tActivation function: {self.g.__name__}\")\n",
    "        print(f\"\\tZ shape: {self.Z.shape}\")\n",
    "        print(f\"\\tA shape: {self.A.shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e32bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batches(data_len: int, batch_size: int = 32) -> List[np.ndarray]:\n",
    "    \"\"\" Generates mini-batches based on the data indexes\n",
    "        \n",
    "        Args:\n",
    "            data_len: Length of the data\n",
    "            \n",
    "            batch_size: Size of each mini batch where the last mini-batch\n",
    "                might be smaller than the rest if the batch_size does not \n",
    "                evenly divide the data length.\n",
    "    \n",
    "    \"\"\"\n",
    "    X_idx = np.arange(data_len)\n",
    "    np.random.shuffle(X_idx)\n",
    "    batches = [X_idx[i:i+batch_size] for i in range(0, data_len, batch_size)]\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12373305",
   "metadata": {},
   "source": [
    "## TODO: Neural Network\n",
    "Complete the TODOs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e47f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(BaseEstimator):\n",
    "    \"\"\" Runs the initialization and training process for a multi-layer neural network.\n",
    "        \n",
    "        Attributes:\n",
    "            neurons_per_layer: A list where each element represents \n",
    "                    the neurons in a layer. For example, [2, 3] would\n",
    "                    create a 2 layer neural network where the hidden layer\n",
    "                    has 2 neurons and the output layer has 3 neurons.\n",
    "            \n",
    "            learning_curve_loss: Pointer to a function which computes the SSE or NLL loss.\n",
    "                This loss will be tracked for each mini-batch and epoch. The loss computed\n",
    "                will be stored in the avg_trn_loss_tracker and avg_vld_loss_tracker variables.\n",
    "\n",
    "            delta_loss_func: Pointer to a function which computes the  derivative for\n",
    "                the MSE or NLL loss.\n",
    "\n",
    "            g_hidden: Activation function used by ALL neurons \n",
    "                in ALL hidden layers.\n",
    "                    \n",
    "            g_output: Activation function used by ALL neurons\n",
    "                in the output layer.\n",
    "        \n",
    "            alpha: learning rate or step size used by gradient descent.\n",
    "                \n",
    "            epochs: Number of times data is used to update the weights `self.w`.\n",
    "                Each epoch means a data sample was used to update the weights at least\n",
    "                once.\n",
    "            \n",
    "            batch_size: Mini-batch size used to determine the size of mini-batches\n",
    "                if mini-batch gradient descent is used.\n",
    "            \n",
    "            seed: Random seed to use when initializing the layers of the neural network.\n",
    "\n",
    "            verbose: If True, print statements inside the train() method will\n",
    "                be printed.\n",
    "\n",
    "            nn: A list of Layer class instances which define the neural network.\n",
    "\n",
    "            avg_trn_loss_tracker: A list that tracks the average training loss per epoch. \n",
    "\n",
    "            avg_vld_loss_tracker: A list that tracks the average validation loss per epoch.\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        neurons_per_layer: List[int],\n",
    "        learning_curve_loss: Callable,\n",
    "        delta_loss_func: Callable,\n",
    "        g_hidden: object,\n",
    "        g_output: object,\n",
    "        alpha: float = .001, \n",
    "        epochs: int = 1, \n",
    "        batch_size: int = 64,\n",
    "        seed: int = None,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.learning_curve_loss = learning_curve_loss\n",
    "        self.delta_loss_func = delta_loss_func\n",
    "        self.g_hidden = g_hidden\n",
    "        self.g_output = g_output\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.nn = []\n",
    "        self.avg_trn_loss_tracker = []\n",
    "        self.avg_vld_loss_tracker = []\n",
    "\n",
    "    def init_neural_network(self, n_input_features: int)-> List[Layer]:\n",
    "        \"\"\" Initializes weights and biases for a multi-layer neural network \n",
    "        \n",
    "            Args:\n",
    "                n_input_features: Number of features the input data has\n",
    "                \n",
    "            TODO:\n",
    "                Finish this method by completing the for loop to initialize the weights\n",
    "                `W` and biases `b`. Once initialized, create an instance of the `Layer`\n",
    "                class by passing the required arguments of weights `W`, biases `b`, \n",
    "                activation function `g`, and name `name` and then append it to the \n",
    "                `nn` list. Return the completed neural network `nn` once the for-loop\n",
    "                has finished.\n",
    "\n",
    "        \"\"\"\n",
    "        nn = []\n",
    "        # Set numpy global random seed\n",
    "        np.random.seed(self.seed)\n",
    "        for l, neurons in enumerate(self.neurons_per_layer):\n",
    "            # Set inputs to number of input features\n",
    "            # for the first hidden layer\n",
    "            if l == 0:\n",
    "                inputs = n_input_features\n",
    "            else:\n",
    "                inputs = self.neurons_per_layer[l-1]\n",
    "            \n",
    "            # Set activation functions for the output\n",
    "            # layer neurons and set the names of the nn\n",
    "            if l == len(self.neurons_per_layer)-1:\n",
    "                g = self.g_output\n",
    "                name = f\"Layer {l+1}: Output Layer\"\n",
    "            else:\n",
    "                g = self.g_hidden\n",
    "                name = f\"Layer {l+1}: Hidden Layer\"\n",
    "            \n",
    "            # Initialize weights and biases\n",
    "            W = self.init_weights(neurons, inputs)\n",
    "            b = np.zeros((neurons, 1))\n",
    "            \n",
    "            # Create layer and add to neural network\n",
    "            layer = Layer(W, b, g, name)\n",
    "            nn.append(layer)\n",
    "            \n",
    "        return nn\n",
    "\n",
    "    def init_weights(self, neurons: int, inputs: int) -> np.ndarray:\n",
    "        \"\"\" Initializes weight values\n",
    "        \n",
    "            Args:\n",
    "                neurons: Number of neurons in the layer\n",
    "                \n",
    "                inputs: Number of inputs to the layer\n",
    "            \n",
    "            TODO:\n",
    "                Finish this method by returning randomly initalized weights given\n",
    "                the arguments for the number of neurons and inputs. Return the randomly\n",
    "                initialized weights once done.\n",
    "        \"\"\"\n",
    "        limit = np.sqrt(6 / (neurons + inputs))\n",
    "        return np.random.uniform(-limit, limit, size=(neurons, inputs))\n",
    "    \n",
    "    def fit(\n",
    "        self, \n",
    "        X: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        X_vld: np.ndarray = None, \n",
    "        y_vld: np.ndarray = None,\n",
    "    ) -> None:\n",
    "        \"\"\" Initializes and trains the defined neural network using gradient descent  \n",
    "        \n",
    "            Args:\n",
    "                X: Training features/data \n",
    "                \n",
    "                y: Training targets/labels\n",
    "\n",
    "                X_vld: validation features/data which are used for computing the validation\n",
    "                    loss after every epoch.\n",
    "\n",
    "                y_vld: validation targets/labels which are used for computing the validation\n",
    "                    loss after every epoch.\n",
    "                    \n",
    "            TODO:\n",
    "                Finish this method by completing the training loop which performs \n",
    "                mini-batch gradient descent and tracks the training loss and validation\n",
    "                scores per each epoch. To complete the training loop, you will need to\n",
    "                initialize the neural network list `nn`, call the forward pass, and call\n",
    "                the backwards pass.\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        self.avg_trn_loss_tracker = []\n",
    "        self.avg_vld_loss_tracker = []\n",
    "        \n",
    "        # TODO (REQUIRED) Initialize self.nn below by replacing []\n",
    "        self.nn = self.init_neural_network(X.shape[1])\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            if self.verbose: print(f\"Epoch: {e+1}\")\n",
    "            batches = get_mini_batches(data_len=m, batch_size=self.batch_size)\n",
    "            total_trn_batch_loss = 0\n",
    "            for mb in batches:\n",
    "                # Forward pass to get predictions\n",
    "                # TODO (REQUIRED) Store the training forward pass predictions below by replacing np.zeros()\n",
    "                y_hat = self.forward(X[mb])\n",
    "\n",
    "                # Backward pass to get gradients\n",
    "                # TODO (REQUIRED) Add backwards pass call below\n",
    "                self.backward(X[mb], y[mb], y_hat)\n",
    "\n",
    "                trn_batch_loss = self.learning_curve_loss(y[mb], y_hat)\n",
    "                total_trn_batch_loss += trn_batch_loss\n",
    "                \n",
    "            avg_trn_loss = total_trn_batch_loss / m\n",
    "            if self.verbose: print(f\"\\tTraining loss: {avg_trn_loss}\")\n",
    "            self.avg_trn_loss_tracker.append(avg_trn_loss)\n",
    "            \n",
    "            if X_vld is not None and y_vld is not None:\n",
    "                m_vld = len(y_vld)\n",
    "                # TODO (REQUIRED) Store the validation forward pass predictions below by replacing np.zeros()\n",
    "                y_hat_vld = self.forward(X_vld)\n",
    "                \n",
    "                avg_vld_loss = self.learning_curve_loss(y_vld, y_hat_vld) / m_vld\n",
    "                if self.verbose: print(f\"\\tValidation loss: {avg_vld_loss}\")\n",
    "                self.avg_vld_loss_tracker.append(avg_vld_loss)\n",
    "            \n",
    "    def forward(self, X:np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Performs the forward pass for a multi-layer neural network\n",
    "    \n",
    "            Args:\n",
    "                X: Input features. This should be typically be the \n",
    "                    training data.\n",
    "                    \n",
    "            TODO: \n",
    "                Finish this method by performing the forward pass for a multi-layer\n",
    "                neural network. Return the output `y_hat` once done.\n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "\n",
    "        A_prev = X\n",
    "        \n",
    "        for layer in self.nn:\n",
    "            layer.Z = np.dot(A_prev, layer.W.T) + layer.b.T\n",
    "            \n",
    "            layer.A = layer.g.activation(layer.Z)\n",
    "            \n",
    "            A_prev = layer.A\n",
    "        \n",
    "        return self.nn[-1].A\n",
    "    \n",
    "    def backward(self, X:np.ndarray, y:np.ndarray, y_hat:np.ndarray) -> None:\n",
    "        \"\"\" Performs the feedback process for a multi-layer neural network\n",
    "        \n",
    "            Args:\n",
    "                X: Training features/data\n",
    "                \n",
    "                y: Training targets/labels\n",
    "                \n",
    "                y_hat: Training predictions (predicted targets or probabilities)\n",
    "\n",
    "            TODO:\n",
    "                Finish this method by performing the backward pass for a multi-layer\n",
    "                neural network.\n",
    "        \"\"\"\n",
    "        m = X.shape[0] \n",
    "        \n",
    "        delta = self.delta_loss_func(y, y_hat)\n",
    "        \n",
    "        for l in reversed(range(len(self.nn))):\n",
    "            layer = self.nn[l]\n",
    "            \n",
    "            if l < len(self.nn) - 1:\n",
    "                next_layer = self.nn[l+1]\n",
    "                delta = np.dot(delta, next_layer.W) * layer.g.derivative(layer.Z)\n",
    "            \n",
    "            if l == 0:\n",
    "                A_prev = X\n",
    "            else:\n",
    "                A_prev = self.nn[l-1].A\n",
    "            \n",
    "            dW = np.dot(delta.T, A_prev) / m\n",
    "            db = np.sum(delta, axis=0, keepdims=True).T / m\n",
    "            \n",
    "            self.nn[l].W -= self.alpha * dW\n",
    "            self.nn[l].b -= self.alpha * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ab5d8",
   "metadata": {},
   "source": [
    "## TODO: Neural Network Regressor\n",
    "Complete the TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2685af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkRegressor(NeuralNetwork):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "        self._param_names = list(kwargs.keys())\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\" Gets all class variables\n",
    "        \n",
    "            This is a a method for compatibility with Sklearn's GridSearchCV \n",
    "        \"\"\"\n",
    "        return {param: getattr(self, param)\n",
    "                for param in self._param_names}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        \"\"\" Sets all class variables\n",
    "        \n",
    "            This is a a method for compatibility with Sklearn's GridSearchCV \n",
    "        \"\"\"\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Make predictions using parameters learned during training.\n",
    "        \n",
    "            Args:\n",
    "                X: Features/data to make predictions with \n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to make a prediction. \n",
    "                Store the predicted labels into `y_hat`.\n",
    "        \"\"\"\n",
    "        # TODO (REQUIRED) Add code below\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "\n",
    "        # TODO (REQUIRED) Store predictions below by replacing np.ones()\n",
    "        y_hat = self.forward(X)\n",
    "        # Makes sure predictions are given as a 2D array\n",
    "        return y_hat.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce03ba0",
   "metadata": {},
   "source": [
    "## TODO: Neural Network Classifier\n",
    "Complete the TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304b4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier(NeuralNetwork):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Code for extracting kwargs and storing them in _param_names\n",
    "        # to be used later with get_params() and set_params() methods\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "        self._param_names = list(kwargs.keys())\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\" Gets all class variables\n",
    "        \n",
    "            This is method is for compatibility with Sklearn's GridSearchCV \n",
    "        \"\"\"\n",
    "        return {param: getattr(self, param)\n",
    "                for param in self._param_names}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        \"\"\" Sets all class variables\n",
    "        \n",
    "            This is method is for compatibility with Sklearn's GridSearchCV \n",
    "        \"\"\"\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict probabilities using parameters learned during training.\n",
    "        \n",
    "            This is method is for compatibility with Sklearn's GridSearchCV \n",
    "                \n",
    "            Args:\n",
    "                X: Features/data to make predictions with \n",
    "\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Make predictions using parameters learned during training.\n",
    "        \n",
    "            Args:\n",
    "                X: Features/data to make predictions with \n",
    "\n",
    "            TODO:\n",
    "                Finish this method by adding code to make a prediction. \n",
    "                Store the predicted labels into `y_hat`.\n",
    "        \"\"\"\n",
    "        # TODO (REQUIRED) Add code below\n",
    "        probs = self.forward(X)\n",
    "         \n",
    "        # TODO (REQUIRED) Store predictions below by replacing np.ones()\n",
    "        y_hat = np.argmax(probs, axis=1)\n",
    "        # Makes sure predictions are given as a 2D array\n",
    "        return y_hat.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff9a88a",
   "metadata": {},
   "source": [
    "# Data preparation and Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c94f5",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "**Don't change this part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7ad7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation():\n",
    "    def __init__(self, target_pipe, feature_pipe):\n",
    "        self.target_pipe = target_pipe\n",
    "        self.feature_pipe = feature_pipe\n",
    "        \n",
    "    @abstractmethod\n",
    "    def data_prep(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.target_pipe  is not None:\n",
    "            self.target_pipe.fit(y)\n",
    "            \n",
    "        if self.feature_pipe is not None:\n",
    "            self.feature_pipe.fit(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.target_pipe is not None:\n",
    "            y = self.target_pipe.transform(y)\n",
    "            \n",
    "        if self.feature_pipe is not None:\n",
    "            X = self.feature_pipe.transform(X)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        X, y = self.transform(X, y)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2da7bd",
   "metadata": {},
   "source": [
    "## Housing Dataset \n",
    "**Don't change this part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4595f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingDataPreparation(DataPreparation):\n",
    "    def __init__(self, target_pipe, feature_pipe, use_features):\n",
    "        super().__init__(target_pipe, feature_pipe)\n",
    "        self.use_features = use_features\n",
    "        \n",
    "    def data_prep(self, return_array=False):\n",
    "        \n",
    "        if self.target_pipe is not None:\n",
    "            warnings.warn(\"Target pipeline is not needed for the Boston House Price dataset. \" \\\n",
    "                          \"Even though you passed a Pipeline for `target_pipe`, \" \\\n",
    "                          \"`target_pipe` will be set to None.\")\n",
    "            self.target_pipe = None\n",
    "        \n",
    "        if return_array: \n",
    "            print(\"Returning data as NumPy array...\")\n",
    "            return_df = False\n",
    "        \n",
    "        print(f\"Attempting to use the following features: {self.use_features}\")\n",
    "        housing_dataset = HousingDataset()\n",
    "        house_df_trn, house_df_vld = housing_dataset.load()\n",
    "        \n",
    "        X_trn_df, y_trn_df, X_vld_df, y_vld_df = split_data(\n",
    "            df_trn=house_df_trn,\n",
    "            df_vld=house_df_vld,\n",
    "            use_features=self.use_features,\n",
    "            label_name='MEDV',\n",
    "            return_df=return_df\n",
    "        )\n",
    "\n",
    "        X_trn_df, y_trn_df = self.fit_transform(X=X_trn_df, y=y_trn_df)\n",
    "        X_vld_df, y_vld_df = self.transform(X=X_vld_df, y=y_vld_df)\n",
    "        \n",
    "        return X_trn_df, y_trn_df, X_vld_df, y_vld_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527811b9",
   "metadata": {},
   "source": [
    "## MNIST Dataset\n",
    "**Don't change this part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "642cc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataPreparation(DataPreparation):\n",
    "    def __init__(self, target_pipe, feature_pipe):\n",
    "        super().__init__(target_pipe, feature_pipe)\n",
    "        \n",
    "    def data_prep(self, binarize=False, return_array=False):\n",
    "        mnist_dataset = MNISTDataset()\n",
    "        X_trn_df, y_trn_df, X_vld_df, y_vld_df = mnist_dataset.load()\n",
    "        \n",
    "        # Converts MNIST problem to classifying ONLY 1s vs 0s\n",
    "        if binarize:\n",
    "            X_trn_df, y_trn_df = binarize_classes(\n",
    "                X_trn_df, \n",
    "                y_trn_df, \n",
    "                pos_class=[1],\n",
    "                neg_class=[0], \n",
    "            )\n",
    "            \n",
    "            X_vld_df, y_vld_df = binarize_classes(\n",
    "                X_vld_df, \n",
    "                y_vld_df, \n",
    "                pos_class=[1], \n",
    "                neg_class=[0], \n",
    "            )\n",
    "\n",
    "        X_trn_df, y_trn_df = self.fit_transform(X=X_trn_df, y=y_trn_df)\n",
    "        X_vld_df, y_vld_df = self.transform(X=X_vld_df, y=y_vld_df)\n",
    "\n",
    "        if return_array:\n",
    "            print(\"Returning data as NumPy array...\")\n",
    "            return dataframe_to_array([X_trn_df, y_trn_df, X_vld_df, y_vld_df])\n",
    "            \n",
    "        return X_trn_df, y_trn_df, X_vld_df, y_vld_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24bda52",
   "metadata": {},
   "source": [
    "## TODO: Define Activation Functions\n",
    "Complete the TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ece8acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_mse(y, y_hat):\n",
    "    # TODO (REQUIRED) Add code below for the derivative of the mean squared error\n",
    "    return y_hat - y\n",
    "\n",
    "def delta_softmax_nll(y, y_hat):\n",
    "    # TODO (REQUIRED) Add code below for for the combined derivative of the softmax and negative log likelihood loss\n",
    "    return y_hat - y\n",
    "\n",
    "class Linear():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        # TODO (REQUIRED) Add code below for Linear activation function equation\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        # TODO (REQUIRED) Add code below for Linear activation function derivative\n",
    "        return np.ones_like(z)\n",
    "    \n",
    "class Sigmoid():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        # TODO (REQUIRED) Add code below for Sigmoid activation function equation\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        # TODO (REQUIRED) Add code below for Sigmoid activation function derivative\n",
    "        sig_z = Sigmoid.activation(z)\n",
    "        return sig_z * (1 - sig_z)\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        # TODO (REQUIRED) Add code below for Tanh activation function equation\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        # TODO (REQUIRED) Add code below for Tanh activation function derivative\n",
    "        return 1 - np.tanh(z)**2\n",
    "\n",
    "class ReLU():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "         # TODO (REQUIRED) Add code below for ReLU activation function equation\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        # TODO (REQUIRED) Add code below for ReLU activation function derivative\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "class Softmax():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        # TODO (REQUIRED) Add code below for softmax activation function equation\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        # TODO (REQUIRED) Add code below for softmax activation function derivative\n",
    "        return np.ones_like(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50bad6e",
   "metadata": {},
   "source": [
    "# TODO: Define Hyperparameters \n",
    "Complete the TODOs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a4e1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParametersAndTransforms():\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_params(name):\n",
    "        model = getattr(HyperParametersAndTransforms, name)\n",
    "        params = {}\n",
    "        for key, value in model.__dict__.items():\n",
    "            if not key.startswith('__') and not callable(key):\n",
    "                if not callable(value) and not isinstance(value, staticmethod):\n",
    "                    params[key] = value\n",
    "        return params\n",
    "    \n",
    "    class NeuralNetworkRegressor():\n",
    "        \"\"\"Kwargs for regression neural network and data prep\"\"\"\n",
    "        model_kwargs = dict(\n",
    "            neurons_per_layer = [32, 24, 16, 8, 1], # TODO (REQUIRED) Set neural network neurons per layer\n",
    "            learning_curve_loss = sse,\n",
    "            delta_loss_func = delta_mse, # TODO (REQUIRED) Set neural network's loss function derivative\n",
    "            g_hidden = ReLU, # TODO (REQUIRED) Set neural network's hidden neurons activation function\n",
    "            g_output = Linear,  # TODO (REQUIRED) Set neural network's output neurons activation function\n",
    "            alpha = 0.0005, # TODO (REQUIRED) Set neural network's learning rate\n",
    "            epochs = 300,  # TODO (REQUIRED) Set neural network's  epochs\n",
    "            batch_size = 8, # TODO (REQUIRED) Set neural network's mini-batch size\n",
    "            verbose = True, # TODO (OPTIONAL) Set to allow neural network to print debugging statements during training \n",
    "            seed = 42, # TODO (OPTIONAL) Set the neural network to random state seed \n",
    "        )\n",
    "        \n",
    "        # (OPTIONAL) model kwargs used for hyper-parameter search.\n",
    "        # EVERY argument must be wrapped in a list.\n",
    "        search_model_kwargs = dict(\n",
    "            neurons_per_layer = [[32, 24, 16, 8, 1], [64, 48, 32, 16, 1]], # TODO (OPTIONAL) Set neural network neurons per layer\n",
    "            learning_curve_loss = [sse],\n",
    "            delta_loss_func = [delta_mse], # TODO (OPTIONAL) Set neural network's loss function derivative\n",
    "            g_hidden = [ReLU], # TODO (OPTIONAL) Set neural network's hidden neurons activation function\n",
    "            g_output = [Linear],  # TODO (OPTIONAL) Set neural network's output neurons activation function\n",
    "            alpha = [0.003, 0.005], # TODO (OPTIONAL) Set neural network's learning rate\n",
    "            epochs = [250, 300],  # TODO (OPTIONAL) Set neural network's  epochs\n",
    "            batch_size = [8, 12], # TODO (OPTIONAL) Set neural network's mini-batch size\n",
    "            verbose = [True], # TODO (OPTIONAL) Set to allow neural network to print debugging statements during training \n",
    "            seed = [42], # TODO (OPTIONAL) Set the neural network to random state seed \n",
    "        )\n",
    "        \n",
    "        data_prep_kwargs = dict(\n",
    "            # TODO (OPTIONAL) Add Pipeline() definitions below\n",
    "            target_pipe = None,\n",
    "            # TODO (REQUIRED) Add Pipeline() definitions below\n",
    "            feature_pipe = Pipeline([\n",
    "                ('standardize', Standardization())\n",
    "            ]),\n",
    "            # TODO (OPTIONAL) Set the names of the features/columns to use for the Housing dataset\n",
    "            use_features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "                            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'],\n",
    "        )\n",
    "        \n",
    "    class NeuralNetworkClassifier():\n",
    "        \"\"\"Kwargs for classifier neural network and data prep\"\"\"\n",
    "        model_kwargs = dict(\n",
    "            neurons_per_layer = [128, 64, 10], # TODO (REQUIRED) Set neural network neurons per layer\n",
    "            learning_curve_loss = nll,\n",
    "            delta_loss_func = delta_softmax_nll, # TODO (REQUIRED) Set neural network's loss function derivative\n",
    "            g_hidden = ReLU, # TODO (REQUIRED) Set neural network's hidden neurons activation function\n",
    "            g_output = Softmax,  # TODO (REQUIRED) Set neural network's output neurons activation function\n",
    "            alpha = 0.01, # TODO (REQUIRED) Set neural network's learning rate\n",
    "            epochs = 10,  # TODO (REQUIRED) Set neural network's  epochs\n",
    "            batch_size = 64, # TODO (REQUIRED) Set neural network's mini-batch size\n",
    "            verbose = True, # TODO (OPTIONAL) Set to allow neural network to print debugging statements during training \n",
    "            seed = 42, # TODO (OPTIONAL) Set the neural network to random state seed \n",
    "        )\n",
    "        \n",
    "        # (OPTIONAL) model kwargs used for hyper-parameter search.\n",
    "        # EVERY argument must be wrapped in a list.\n",
    "        search_model_kwargs = dict(\n",
    "            neurons_per_layer = [[128, 64, 10], [256, 128, 10]], # TODO (OPTIONAL) Set neural network neurons per layer\n",
    "            learning_curve_loss = [nll],\n",
    "            delta_loss_func = [delta_softmax_nll], # TODO (OPTIONAL) Set neural network's loss function derivative\n",
    "            g_hidden = [ReLU, Tanh], # TODO (OPTIONAL) Set neural network's hidden neurons activation function\n",
    "            g_output = [Softmax],  # TODO (OPTIONAL) Set neural network's output neurons activation function\n",
    "            alpha = [0.01, 0.005], # TODO (OPTIONAL) Set neural network's learning rate\n",
    "            epochs = [10, 15],  # TODO (OPTIONAL) Set neural network's  epochs\n",
    "            batch_size = [64, 128], # TODO (OPTIONAL) Set neural network's mini-batch size\n",
    "            verbose = [True], # TODO (OPTIONAL) Set to allow neural network to print debugging statements during training \n",
    "            seed = [42], # TODO (OPTIONAL) Set the neural network to random state seed \n",
    "        )\n",
    "        \n",
    "        data_prep_kwargs = dict(\n",
    "            target_pipe = Pipeline([\n",
    "                ('one_hot', OneHotEncoding())\n",
    "            ]),\n",
    "            feature_pipe = Pipeline([\n",
    "                ('normalize', ImageNormalization())\n",
    "            ]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dfc018",
   "metadata": {},
   "source": [
    "## Define Model running (training/fit and testing/evaluate)\n",
    "**Don't change this part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "843c6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(obj):\n",
    "    try:\n",
    "        if hasattr(obj, '__name__'):\n",
    "            return obj.__name__\n",
    "        else:\n",
    "            return obj\n",
    "    except Exception as e:\n",
    "        return obj\n",
    "    \n",
    "def catch_and_throw(e, err):\n",
    "    trace = traceback.format_exc()\n",
    "    print(err + f\"\\n{trace}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d3c43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunModel():\n",
    "    t1 = '\\t'\n",
    "    t2 = '\\t\\t'\n",
    "    t3 = '\\t\\t\\t'\n",
    "    def __init__(self, model, model_params):\n",
    "        self.model_name = model.__name__\n",
    "        self.model_params = model_params\n",
    "        self.model = self.build_model(model, model_params)\n",
    "\n",
    "    def build_model(self, model, model_params):\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Building model {self.model_name}\")\n",
    "        \n",
    "        try:\n",
    "            model = model(**model_params)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while building model for {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        print(f\"Training {self.model_name}...\")\n",
    "        print(f\"{self.t1}Using hyperparameters: \")\n",
    "        [print(f\"{self.t2}{n} = {get_name(v)}\")for n, v in self.model_params.items()]\n",
    "        try: \n",
    "            return self._fit(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while training model for {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "            \n",
    "    def _fit(self, X, y, metrics=None, pass_y=False):\n",
    "        if pass_y:\n",
    "            self.model.fit(X, y)\n",
    "        else:\n",
    "             self.model.fit(X)\n",
    "        preds = self.model.predict(X)\n",
    "        scores = self.get_metrics(y, preds, metrics, prefix='Train')\n",
    "        return scores\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        print(f\"Evaluating {self.model_name}...\")\n",
    "        try:\n",
    "            return self._evaluate(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while evaluating model for {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "        \n",
    "\n",
    "    def _evaluate(self, X, y, metrics, prefix=''):\n",
    "        preds = self.model.predict(X)\n",
    "        scores = self.get_metrics(y, preds, metrics, prefix)      \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        try:\n",
    "            preds = self.model.predict(X)\n",
    "        except Exception as e:\n",
    "            err = f\"Exception caught while making predictions for model {self.model_name}:\"\n",
    "            catch_and_throw(e, err)\n",
    "            \n",
    "        return preds\n",
    "    \n",
    "    def get_metrics(self, y, y_hat, metrics, prefix=''):\n",
    "        scores = {}\n",
    "        for name, metric in metrics.items():\n",
    "            score = metric(y, y_hat)\n",
    "            display_score = round(score, 3)\n",
    "            scores[name] = score\n",
    "            print(f\"{self.t2}{prefix} {name}: {display_score}\")\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e122ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(eval_stage='validation'):\n",
    "    main_timer = Timer()\n",
    "    main_timer.start()\n",
    "    \n",
    "    task_info = [\n",
    "       dict(\n",
    "            model=NeuralNetworkRegressor,\n",
    "            name='NeuralNetworkRegressor',\n",
    "            data=HousingDataPreparation,\n",
    "            data_prep=dict(return_array=True),\n",
    "            metrics=dict(mse=mse),\n",
    "            eval_metric='mse',\n",
    "            rubric=rubric_regression,\n",
    "            trn_score=9999,\n",
    "            eval_score=9999,\n",
    "            successful=False,\n",
    "        ),\n",
    "        dict(\n",
    "            model=NeuralNetworkClassifier,\n",
    "            name='NeuralNetworkClassifier',\n",
    "            data=MNISTDataPreparation,\n",
    "            data_prep=dict(return_array=True),\n",
    "            metrics=dict(acc=accuracy),\n",
    "            eval_metric='acc',\n",
    "            rubric=rubric_classification,\n",
    "            trn_score=0,\n",
    "            eval_score=0,\n",
    "            successful=False,\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    total_points = 0\n",
    "\n",
    "    for info in task_info:\n",
    "        task_timer =  Timer()\n",
    "        task_timer.start()\n",
    "        try:\n",
    "            params = HyperParametersAndTransforms.get_params(info['name'])\n",
    "            model_kwargs = params.get('model_kwargs', {})\n",
    "            data_prep_kwargs = params.get('data_prep_kwargs', {})\n",
    "            \n",
    "            run_model = RunModel(info['model'], model_kwargs)\n",
    "            data = info['data'](**data_prep_kwargs)\n",
    "            X_trn, y_trn, X_vld, y_vld = data.data_prep(**info['data_prep'])\n",
    "\n",
    "            trn_scores = run_model.fit(X_trn, y_trn, info['metrics'], pass_y=True)\n",
    "            eval_scores = run_model.evaluate(X_vld, y_vld, info['metrics'], prefix=eval_stage.capitalize())\n",
    "            \n",
    "            if not math.isnan(trn_scores[info['eval_metric']]):\n",
    "                info['trn_score'] = trn_scores[info['eval_metric']]\n",
    "            if not math.isnan(eval_scores[info['eval_metric']]):\n",
    "                info['eval_score'] = eval_scores[info['eval_metric']]\n",
    "            \n",
    "            info['successful'] = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            track = traceback.format_exc()\n",
    "            print(\"The following exception occurred while executing this test case:\\n\", track)\n",
    "        task_timer.stop()\n",
    "        \n",
    "        print(\"\")\n",
    "        points = info['rubric'](info['eval_score'])\n",
    "        print(f\"Points Earned: {points}\")\n",
    "        total_points += points\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print('')\n",
    "    main_timer.stop()\n",
    "\n",
    "    successful_tests = summary(task_info)\n",
    "    final_mse, final_acc = get_eval_scores(task_info)\n",
    "    total_points = int(round(total_points))\n",
    "    \n",
    "    print(f\"Tests passed: {successful_tests}/{ len(task_info)}, Total Points: {total_points}/80\\n\")\n",
    "    print(f\"Final {eval_stage.capitalize()} MSE: {final_mse}\")\n",
    "    print(f\"Final {eval_stage.capitalize()} Accuracy: {final_acc}\")\n",
    "\n",
    "    return total_points, main_timer.last_elapsed_time, final_mse, final_acc\n",
    "\n",
    "def summary(task_info):\n",
    "    successful_tests = 0\n",
    "\n",
    "    for info in task_info:\n",
    "        if info['successful']:\n",
    "            successful_tests += 1\n",
    "    \n",
    "    if successful_tests == 0:\n",
    "        return successful_tests\n",
    "\n",
    "    return successful_tests\n",
    "\n",
    "def get_eval_scores(task_info):\n",
    "    return [i['eval_score'] for i in task_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d7e19",
   "metadata": {},
   "source": [
    "## Evaluation Related Functions\n",
    "Don't change this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0e86c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rubric_regression(mse, max_score=40):\n",
    "    thresh = 12.5\n",
    "    if mse <= thresh:\n",
    "        score_percent = 100\n",
    "    elif mse is not None:\n",
    "        score_percent = (thresh / mse) * 100\n",
    "        if score_percent < 40:\n",
    "            score_percent = 40\n",
    "    else:\n",
    "        score_percent = 20\n",
    "    score = max_score * score_percent / 100.0\n",
    "\n",
    "    return score\n",
    "\n",
    "def rubric_classification(acc, max_score=40):\n",
    "    score_percent = 0\n",
    "    if acc >= 0.93:\n",
    "        score_percent = 100\n",
    "    elif acc >= 0.85:\n",
    "        score_percent = 90\n",
    "    elif acc >= 0.70:\n",
    "        score_percent = 80\n",
    "    elif acc >= 0.60:\n",
    "        score_percent = 70\n",
    "    elif acc >= 0.50:\n",
    "        score_percent = 60\n",
    "    elif acc >= 0.40:\n",
    "        score_percent = 55\n",
    "    elif acc >= 0.30:\n",
    "        score_percent = 50\n",
    "    elif acc >= 0.20:\n",
    "        score_percent = 45\n",
    "    else:\n",
    "        score_percent = 40\n",
    "    score = max_score * score_percent / 100.0 \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d29e0b",
   "metadata": {},
   "source": [
    "# Test your code\n",
    "Run the following cell to test your code (or for **debugging**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95536ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Building model NeuralNetworkRegressor\n",
      "Returning data as NumPy array...\n",
      "Attempting to use the following features: ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
      "Skipping download. File already exists: /Users/hritz/Downloads/Neural Network Mini Project/datasets/data/housing.train\n",
      "\n",
      "Skipping download. File already exists: /Users/hritz/Downloads/Neural Network Mini Project/datasets/data/housing.val\n",
      "\n",
      "Skipping download. File already exists: /Users/hritz/Downloads/Neural Network Mini Project/datasets/data/housing.names\n",
      "\n",
      "Training NeuralNetworkRegressor...\n",
      "\tUsing hyperparameters: \n",
      "\t\tneurons_per_layer = [32, 24, 16, 8, 1]\n",
      "\t\tlearning_curve_loss = sse\n",
      "\t\tdelta_loss_func = delta_mse\n",
      "\t\tg_hidden = ReLU\n",
      "\t\tg_output = Linear\n",
      "\t\talpha = 0.0005\n",
      "\t\tepochs = 300\n",
      "\t\tbatch_size = 8\n",
      "\t\tverbose = True\n",
      "\t\tseed = 42\n",
      "Epoch: 1\n",
      "\tTraining loss: 436.2530226754229\n",
      "Epoch: 2\n",
      "\tTraining loss: 33.964219642969454\n",
      "Epoch: 3\n",
      "\tTraining loss: 19.841622259736624\n",
      "Epoch: 4\n",
      "\tTraining loss: 17.222282004362533\n",
      "Epoch: 5\n",
      "\tTraining loss: 16.406243412794375\n",
      "Epoch: 6\n",
      "\tTraining loss: 16.230164950807705\n",
      "Epoch: 7\n",
      "\tTraining loss: 14.52090354283498\n",
      "Epoch: 8\n",
      "\tTraining loss: 14.760194752231223\n",
      "Epoch: 9\n",
      "\tTraining loss: 14.187703680079718\n",
      "Epoch: 10\n",
      "\tTraining loss: 13.394576384095897\n",
      "Epoch: 11\n",
      "\tTraining loss: 13.822894698785294\n",
      "Epoch: 12\n",
      "\tTraining loss: 13.756244779459434\n",
      "Epoch: 13\n",
      "\tTraining loss: 12.47770984884639\n",
      "Epoch: 14\n",
      "\tTraining loss: 13.221859946978281\n",
      "Epoch: 15\n",
      "\tTraining loss: 11.555788693235604\n",
      "Epoch: 16\n",
      "\tTraining loss: 11.411752334053642\n",
      "Epoch: 17\n",
      "\tTraining loss: 11.314101782750008\n",
      "Epoch: 18\n",
      "\tTraining loss: 11.883866808243226\n",
      "Epoch: 19\n",
      "\tTraining loss: 10.95114019657349\n",
      "Epoch: 20\n",
      "\tTraining loss: 11.057625794065382\n",
      "Epoch: 21\n",
      "\tTraining loss: 11.138877468948623\n",
      "Epoch: 22\n",
      "\tTraining loss: 10.261873391732859\n",
      "Epoch: 23\n",
      "\tTraining loss: 10.380783929223892\n",
      "Epoch: 24\n",
      "\tTraining loss: 10.128085758738282\n",
      "Epoch: 25\n",
      "\tTraining loss: 9.830015997431488\n",
      "Epoch: 26\n",
      "\tTraining loss: 9.56055701436414\n",
      "Epoch: 27\n",
      "\tTraining loss: 10.110837221255547\n",
      "Epoch: 28\n",
      "\tTraining loss: 9.336995236801037\n",
      "Epoch: 29\n",
      "\tTraining loss: 9.054100599257973\n",
      "Epoch: 30\n",
      "\tTraining loss: 9.488227691897121\n",
      "Epoch: 31\n",
      "\tTraining loss: 9.453199146592578\n",
      "Epoch: 32\n",
      "\tTraining loss: 8.581673773531582\n",
      "Epoch: 33\n",
      "\tTraining loss: 8.818074074586992\n",
      "Epoch: 34\n",
      "\tTraining loss: 8.465304657064003\n",
      "Epoch: 35\n",
      "\tTraining loss: 8.662276255509317\n",
      "Epoch: 36\n",
      "\tTraining loss: 8.667801258317768\n",
      "Epoch: 37\n",
      "\tTraining loss: 8.247102952825388\n",
      "Epoch: 38\n",
      "\tTraining loss: 8.278778223529265\n",
      "Epoch: 39\n",
      "\tTraining loss: 8.147500714837376\n",
      "Epoch: 40\n",
      "\tTraining loss: 8.203564793270823\n",
      "Epoch: 41\n",
      "\tTraining loss: 7.926178818909609\n",
      "Epoch: 42\n",
      "\tTraining loss: 8.154762926408921\n",
      "Epoch: 43\n",
      "\tTraining loss: 8.028890344294235\n",
      "Epoch: 44\n",
      "\tTraining loss: 7.769321889208614\n",
      "Epoch: 45\n",
      "\tTraining loss: 7.440523829044362\n",
      "Epoch: 46\n",
      "\tTraining loss: 7.1850366313683764\n",
      "Epoch: 47\n",
      "\tTraining loss: 7.86212970787178\n",
      "Epoch: 48\n",
      "\tTraining loss: 7.314140465927621\n",
      "Epoch: 49\n",
      "\tTraining loss: 7.733359541194182\n",
      "Epoch: 50\n",
      "\tTraining loss: 7.277748339413346\n",
      "Epoch: 51\n",
      "\tTraining loss: 7.092577646094814\n",
      "Epoch: 52\n",
      "\tTraining loss: 7.0889475066535335\n",
      "Epoch: 53\n",
      "\tTraining loss: 6.634475769992812\n",
      "Epoch: 54\n",
      "\tTraining loss: 6.958179035977462\n",
      "Epoch: 55\n",
      "\tTraining loss: 7.159023446322283\n",
      "Epoch: 56\n",
      "\tTraining loss: 7.235453774173987\n",
      "Epoch: 57\n",
      "\tTraining loss: 6.849829830196805\n",
      "Epoch: 58\n",
      "\tTraining loss: 7.081313560158954\n",
      "Epoch: 59\n",
      "\tTraining loss: 6.8234816960106155\n",
      "Epoch: 60\n",
      "\tTraining loss: 6.701515239943931\n",
      "Epoch: 61\n",
      "\tTraining loss: 6.741182120125763\n",
      "Epoch: 62\n",
      "\tTraining loss: 6.628280268533537\n",
      "Epoch: 63\n",
      "\tTraining loss: 6.441850055170603\n",
      "Epoch: 64\n",
      "\tTraining loss: 6.427418162062232\n",
      "Epoch: 65\n",
      "\tTraining loss: 6.28362851830478\n",
      "Epoch: 66\n",
      "\tTraining loss: 6.210875150930382\n",
      "Epoch: 67\n",
      "\tTraining loss: 6.067813827192068\n",
      "Epoch: 68\n",
      "\tTraining loss: 5.967363352475099\n",
      "Epoch: 69\n",
      "\tTraining loss: 5.994415550661743\n",
      "Epoch: 70\n",
      "\tTraining loss: 5.94329849071823\n",
      "Epoch: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hritz/Downloads/Neural Network Mini Project/datasets/HousingDataset.py:35: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_train = pd.read_csv(self.data[\"paths\"][\"train\"], delim_whitespace=True,\n",
      "/Users/hritz/Downloads/Neural Network Mini Project/datasets/HousingDataset.py:38: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_val = pd.read_csv(self.data[\"paths\"][\"val\"], delim_whitespace=True,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 6.127995743982819\n",
      "Epoch: 72\n",
      "\tTraining loss: 5.588752815241071\n",
      "Epoch: 73\n",
      "\tTraining loss: 5.941494518202342\n",
      "Epoch: 74\n",
      "\tTraining loss: 5.604765366103625\n",
      "Epoch: 75\n",
      "\tTraining loss: 5.849081147445295\n",
      "Epoch: 76\n",
      "\tTraining loss: 5.590917878698715\n",
      "Epoch: 77\n",
      "\tTraining loss: 5.634074394172415\n",
      "Epoch: 78\n",
      "\tTraining loss: 5.545277458262198\n",
      "Epoch: 79\n",
      "\tTraining loss: 5.512230540699442\n",
      "Epoch: 80\n",
      "\tTraining loss: 5.546890845015759\n",
      "Epoch: 81\n",
      "\tTraining loss: 5.524129439590434\n",
      "Epoch: 82\n",
      "\tTraining loss: 5.225087362185905\n",
      "Epoch: 83\n",
      "\tTraining loss: 5.696768724014329\n",
      "Epoch: 84\n",
      "\tTraining loss: 5.324653516479295\n",
      "Epoch: 85\n",
      "\tTraining loss: 5.491525503581388\n",
      "Epoch: 86\n",
      "\tTraining loss: 5.182119657083438\n",
      "Epoch: 87\n",
      "\tTraining loss: 5.122268769409226\n",
      "Epoch: 88\n",
      "\tTraining loss: 5.1853943130195\n",
      "Epoch: 89\n",
      "\tTraining loss: 5.185666082263679\n",
      "Epoch: 90\n",
      "\tTraining loss: 5.231731196501061\n",
      "Epoch: 91\n",
      "\tTraining loss: 5.115134523418452\n",
      "Epoch: 92\n",
      "\tTraining loss: 4.919209278211144\n",
      "Epoch: 93\n",
      "\tTraining loss: 4.771350027807817\n",
      "Epoch: 94\n",
      "\tTraining loss: 5.11587509305384\n",
      "Epoch: 95\n",
      "\tTraining loss: 4.898652679305741\n",
      "Epoch: 96\n",
      "\tTraining loss: 4.801802510739242\n",
      "Epoch: 97\n",
      "\tTraining loss: 4.727029901699724\n",
      "Epoch: 98\n",
      "\tTraining loss: 4.678757091797128\n",
      "Epoch: 99\n",
      "\tTraining loss: 4.6906137739593525\n",
      "Epoch: 100\n",
      "\tTraining loss: 4.679723847654281\n",
      "Epoch: 101\n",
      "\tTraining loss: 4.707309117221998\n",
      "Epoch: 102\n",
      "\tTraining loss: 4.401047897848121\n",
      "Epoch: 103\n",
      "\tTraining loss: 4.682253510330866\n",
      "Epoch: 104\n",
      "\tTraining loss: 4.575198823415832\n",
      "Epoch: 105\n",
      "\tTraining loss: 4.511557123142636\n",
      "Epoch: 106\n",
      "\tTraining loss: 4.46156551480494\n",
      "Epoch: 107\n",
      "\tTraining loss: 4.534654733117664\n",
      "Epoch: 108\n",
      "\tTraining loss: 4.3312020652069405\n",
      "Epoch: 109\n",
      "\tTraining loss: 4.302665129654752\n",
      "Epoch: 110\n",
      "\tTraining loss: 4.3013522063644745\n",
      "Epoch: 111\n",
      "\tTraining loss: 4.453335188064416\n",
      "Epoch: 112\n",
      "\tTraining loss: 4.187575088277522\n",
      "Epoch: 113\n",
      "\tTraining loss: 4.173576691601237\n",
      "Epoch: 114\n",
      "\tTraining loss: 4.233830663542889\n",
      "Epoch: 115\n",
      "\tTraining loss: 4.152986542680331\n",
      "Epoch: 116\n",
      "\tTraining loss: 4.207944530465869\n",
      "Epoch: 117\n",
      "\tTraining loss: 3.98511238698966\n",
      "Epoch: 118\n",
      "\tTraining loss: 4.160705253215487\n",
      "Epoch: 119\n",
      "\tTraining loss: 4.14559395835444\n",
      "Epoch: 120\n",
      "\tTraining loss: 3.998107417353807\n",
      "Epoch: 121\n",
      "\tTraining loss: 4.0049718357168365\n",
      "Epoch: 122\n",
      "\tTraining loss: 4.22876116122862\n",
      "Epoch: 123\n",
      "\tTraining loss: 4.000764221809173\n",
      "Epoch: 124\n",
      "\tTraining loss: 3.9839460450243145\n",
      "Epoch: 125\n",
      "\tTraining loss: 3.828298589646459\n",
      "Epoch: 126\n",
      "\tTraining loss: 3.8427535670108846\n",
      "Epoch: 127\n",
      "\tTraining loss: 3.8434071344283796\n",
      "Epoch: 128\n",
      "\tTraining loss: 3.853743984921917\n",
      "Epoch: 129\n",
      "\tTraining loss: 3.8577116052032547\n",
      "Epoch: 130\n",
      "\tTraining loss: 3.6819932346529\n",
      "Epoch: 131\n",
      "\tTraining loss: 3.6724418330141533\n",
      "Epoch: 132\n",
      "\tTraining loss: 3.691683065123201\n",
      "Epoch: 133\n",
      "\tTraining loss: 3.6745953213323337\n",
      "Epoch: 134\n",
      "\tTraining loss: 3.5423164504052385\n",
      "Epoch: 135\n",
      "\tTraining loss: 3.5875093717611977\n",
      "Epoch: 136\n",
      "\tTraining loss: 3.578336560887396\n",
      "Epoch: 137\n",
      "\tTraining loss: 3.6394129543214544\n",
      "Epoch: 138\n",
      "\tTraining loss: 3.5451776709163965\n",
      "Epoch: 139\n",
      "\tTraining loss: 3.750804581613989\n",
      "Epoch: 140\n",
      "\tTraining loss: 3.5673720799067876\n",
      "Epoch: 141\n",
      "\tTraining loss: 3.5134484382062108\n",
      "Epoch: 142\n",
      "\tTraining loss: 3.5100207122281843\n",
      "Epoch: 143\n",
      "\tTraining loss: 3.3403493903825074\n",
      "Epoch: 144\n",
      "\tTraining loss: 3.4264996109702435\n",
      "Epoch: 145\n",
      "\tTraining loss: 3.3350430817441334\n",
      "Epoch: 146\n",
      "\tTraining loss: 3.5063976268742163\n",
      "Epoch: 147\n",
      "\tTraining loss: 3.455406118127961\n",
      "Epoch: 148\n",
      "\tTraining loss: 3.298868789906582\n",
      "Epoch: 149\n",
      "\tTraining loss: 3.4607191387407665\n",
      "Epoch: 150\n",
      "\tTraining loss: 3.4551751364406504\n",
      "Epoch: 151\n",
      "\tTraining loss: 3.244536428549164\n",
      "Epoch: 152\n",
      "\tTraining loss: 3.463480383247023\n",
      "Epoch: 153\n",
      "\tTraining loss: 3.2514003013818136\n",
      "Epoch: 154\n",
      "\tTraining loss: 3.3936351141971257\n",
      "Epoch: 155\n",
      "\tTraining loss: 3.299916929470342\n",
      "Epoch: 156\n",
      "\tTraining loss: 3.2330937462511815\n",
      "Epoch: 157\n",
      "\tTraining loss: 3.1515648013338358\n",
      "Epoch: 158\n",
      "\tTraining loss: 3.1897536885446107\n",
      "Epoch: 159\n",
      "\tTraining loss: 3.171246082922675\n",
      "Epoch: 160\n",
      "\tTraining loss: 3.23940765377419\n",
      "Epoch: 161\n",
      "\tTraining loss: 3.349354010548179\n",
      "Epoch: 162\n",
      "\tTraining loss: 3.2782308750389375\n",
      "Epoch: 163\n",
      "\tTraining loss: 3.0513413013262256\n",
      "Epoch: 164\n",
      "\tTraining loss: 3.1637701482160514\n",
      "Epoch: 165\n",
      "\tTraining loss: 3.0219638996325267\n",
      "Epoch: 166\n",
      "\tTraining loss: 3.175408965368463\n",
      "Epoch: 167\n",
      "\tTraining loss: 3.144774702176346\n",
      "Epoch: 168\n",
      "\tTraining loss: 3.2148486560850613\n",
      "Epoch: 169\n",
      "\tTraining loss: 3.0742786617555993\n",
      "Epoch: 170\n",
      "\tTraining loss: 3.1306324069075853\n",
      "Epoch: 171\n",
      "\tTraining loss: 3.0078140415616996\n",
      "Epoch: 172\n",
      "\tTraining loss: 3.028420776553599\n",
      "Epoch: 173\n",
      "\tTraining loss: 3.0137727927317464\n",
      "Epoch: 174\n",
      "\tTraining loss: 3.031682472981478\n",
      "Epoch: 175\n",
      "\tTraining loss: 3.100410736691971\n",
      "Epoch: 176\n",
      "\tTraining loss: 3.075915026555073\n",
      "Epoch: 177\n",
      "\tTraining loss: 3.1541824123963833\n",
      "Epoch: 178\n",
      "\tTraining loss: 2.9671782895738428\n",
      "Epoch: 179\n",
      "\tTraining loss: 2.9158818782621183\n",
      "Epoch: 180\n",
      "\tTraining loss: 3.102409581804155\n",
      "Epoch: 181\n",
      "\tTraining loss: 2.9502788080022526\n",
      "Epoch: 182\n",
      "\tTraining loss: 2.961452891980953\n",
      "Epoch: 183\n",
      "\tTraining loss: 2.9970535863727603\n",
      "Epoch: 184\n",
      "\tTraining loss: 3.125537545218322\n",
      "Epoch: 185\n",
      "\tTraining loss: 3.0419053228129553\n",
      "Epoch: 186\n",
      "\tTraining loss: 2.837146261818622\n",
      "Epoch: 187\n",
      "\tTraining loss: 2.823842515599409\n",
      "Epoch: 188\n",
      "\tTraining loss: 2.7040220926841223\n",
      "Epoch: 189\n",
      "\tTraining loss: 2.890850890392987\n",
      "Epoch: 190\n",
      "\tTraining loss: 3.003086060265485\n",
      "Epoch: 191\n",
      "\tTraining loss: 2.9621725228758367\n",
      "Epoch: 192\n",
      "\tTraining loss: 2.8914370062668553\n",
      "Epoch: 193\n",
      "\tTraining loss: 2.725664874197644\n",
      "Epoch: 194\n",
      "\tTraining loss: 2.7783431004737573\n",
      "Epoch: 195\n",
      "\tTraining loss: 2.8882854124217663\n",
      "Epoch: 196\n",
      "\tTraining loss: 2.758261528839694\n",
      "Epoch: 197\n",
      "\tTraining loss: 2.812665604497059\n",
      "Epoch: 198\n",
      "\tTraining loss: 2.8466306269285853\n",
      "Epoch: 199\n",
      "\tTraining loss: 2.7946937508983023\n",
      "Epoch: 200\n",
      "\tTraining loss: 2.693131482919005\n",
      "Epoch: 201\n",
      "\tTraining loss: 2.7789846936710005\n",
      "Epoch: 202\n",
      "\tTraining loss: 2.761069042606451\n",
      "Epoch: 203\n",
      "\tTraining loss: 2.7978177266328594\n",
      "Epoch: 204\n",
      "\tTraining loss: 2.7985676078120187\n",
      "Epoch: 205\n",
      "\tTraining loss: 2.734055508429601\n",
      "Epoch: 206\n",
      "\tTraining loss: 2.792834946666955\n",
      "Epoch: 207\n",
      "\tTraining loss: 2.655808361041186\n",
      "Epoch: 208\n",
      "\tTraining loss: 2.776315600463862\n",
      "Epoch: 209\n",
      "\tTraining loss: 2.7264880581041364\n",
      "Epoch: 210\n",
      "\tTraining loss: 2.693153783806823\n",
      "Epoch: 211\n",
      "\tTraining loss: 2.74744738719271\n",
      "Epoch: 212\n",
      "\tTraining loss: 2.6911996444047332\n",
      "Epoch: 213\n",
      "\tTraining loss: 2.8252314181537077\n",
      "Epoch: 214\n",
      "\tTraining loss: 2.686883319350949\n",
      "Epoch: 215\n",
      "\tTraining loss: 2.549936039313532\n",
      "Epoch: 216\n",
      "\tTraining loss: 2.7074315395056576\n",
      "Epoch: 217\n",
      "\tTraining loss: 2.6418169226535677\n",
      "Epoch: 218\n",
      "\tTraining loss: 2.583911833766755\n",
      "Epoch: 219\n",
      "\tTraining loss: 2.4819628674854495\n",
      "Epoch: 220\n",
      "\tTraining loss: 2.8288042480229882\n",
      "Epoch: 221\n",
      "\tTraining loss: 2.500717773000872\n",
      "Epoch: 222\n",
      "\tTraining loss: 2.6262939372941596\n",
      "Epoch: 223\n",
      "\tTraining loss: 2.854724863113916\n",
      "Epoch: 224\n",
      "\tTraining loss: 2.645897669402279\n",
      "Epoch: 225\n",
      "\tTraining loss: 2.719268470083376\n",
      "Epoch: 226\n",
      "\tTraining loss: 2.7274229436029023\n",
      "Epoch: 227\n",
      "\tTraining loss: 2.698476725178098\n",
      "Epoch: 228\n",
      "\tTraining loss: 2.432024753346131\n",
      "Epoch: 229\n",
      "\tTraining loss: 2.5499136426881357\n",
      "Epoch: 230\n",
      "\tTraining loss: 2.4749210018232852\n",
      "Epoch: 231\n",
      "\tTraining loss: 2.684171395294763\n",
      "Epoch: 232\n",
      "\tTraining loss: 2.4667808683652384\n",
      "Epoch: 233\n",
      "\tTraining loss: 2.5846793676768627\n",
      "Epoch: 234\n",
      "\tTraining loss: 2.5088973133241765\n",
      "Epoch: 235\n",
      "\tTraining loss: 2.4539739444907984\n",
      "Epoch: 236\n",
      "\tTraining loss: 2.959677961936534\n",
      "Epoch: 237\n",
      "\tTraining loss: 2.5140583734572584\n",
      "Epoch: 238\n",
      "\tTraining loss: 2.5292803257672998\n",
      "Epoch: 239\n",
      "\tTraining loss: 2.445943420428187\n",
      "Epoch: 240\n",
      "\tTraining loss: 2.406739225512201\n",
      "Epoch: 241\n",
      "\tTraining loss: 2.3954117377808726\n",
      "Epoch: 242\n",
      "\tTraining loss: 2.3795164701311178\n",
      "Epoch: 243\n",
      "\tTraining loss: 2.4707508887213536\n",
      "Epoch: 244\n",
      "\tTraining loss: 2.716346195547019\n",
      "Epoch: 245\n",
      "\tTraining loss: 2.466362670155966\n",
      "Epoch: 246\n",
      "\tTraining loss: 2.418096584381775\n",
      "Epoch: 247\n",
      "\tTraining loss: 2.4709532210814977\n",
      "Epoch: 248\n",
      "\tTraining loss: 2.4171738443458475\n",
      "Epoch: 249\n",
      "\tTraining loss: 2.422779931417855\n",
      "Epoch: 250\n",
      "\tTraining loss: 2.5600650090648314\n",
      "Epoch: 251\n",
      "\tTraining loss: 2.35254992808128\n",
      "Epoch: 252\n",
      "\tTraining loss: 2.3461727311625986\n",
      "Epoch: 253\n",
      "\tTraining loss: 2.318656985410271\n",
      "Epoch: 254\n",
      "\tTraining loss: 2.62690505828403\n",
      "Epoch: 255\n",
      "\tTraining loss: 2.432819775345647\n",
      "Epoch: 256\n",
      "\tTraining loss: 2.4851797082613083\n",
      "Epoch: 257\n",
      "\tTraining loss: 2.3464261460834823\n",
      "Epoch: 258\n",
      "\tTraining loss: 2.4622614853221965\n",
      "Epoch: 259\n",
      "\tTraining loss: 2.437885044923304\n",
      "Epoch: 260\n",
      "\tTraining loss: 2.3911619561266093\n",
      "Epoch: 261\n",
      "\tTraining loss: 2.4537114922760757\n",
      "Epoch: 262\n",
      "\tTraining loss: 2.7101043089560193\n",
      "Epoch: 263\n",
      "\tTraining loss: 2.348078508711984\n",
      "Epoch: 264\n",
      "\tTraining loss: 2.372635602084595\n",
      "Epoch: 265\n",
      "\tTraining loss: 2.4690404140160993\n",
      "Epoch: 266\n",
      "\tTraining loss: 2.3714340331443906\n",
      "Epoch: 267\n",
      "\tTraining loss: 2.333242117566658\n",
      "Epoch: 268\n",
      "\tTraining loss: 2.4797797008607003\n",
      "Epoch: 269\n",
      "\tTraining loss: 2.369249875253533\n",
      "Epoch: 270\n",
      "\tTraining loss: 2.489200530597051\n",
      "Epoch: 271\n",
      "\tTraining loss: 2.2722802905967554\n",
      "Epoch: 272\n",
      "\tTraining loss: 2.3473391585147874\n",
      "Epoch: 273\n",
      "\tTraining loss: 2.2808257179520592\n",
      "Epoch: 274\n",
      "\tTraining loss: 2.2816535605762263\n",
      "Epoch: 275\n",
      "\tTraining loss: 2.301939476461958\n",
      "Epoch: 276\n",
      "\tTraining loss: 2.443901219614156\n",
      "Epoch: 277\n",
      "\tTraining loss: 2.1184420614308404\n",
      "Epoch: 278\n",
      "\tTraining loss: 2.2047476316967582\n",
      "Epoch: 279\n",
      "\tTraining loss: 2.365529212654476\n",
      "Epoch: 280\n",
      "\tTraining loss: 2.3331737148097345\n",
      "Epoch: 281\n",
      "\tTraining loss: 2.196454214576925\n",
      "Epoch: 282\n",
      "\tTraining loss: 2.3043697070815043\n",
      "Epoch: 283\n",
      "\tTraining loss: 2.268541309420809\n",
      "Epoch: 284\n",
      "\tTraining loss: 2.1913931012666428\n",
      "Epoch: 285\n",
      "\tTraining loss: 2.203514015361502\n",
      "Epoch: 286\n",
      "\tTraining loss: 2.1809945645967805\n",
      "Epoch: 287\n",
      "\tTraining loss: 2.306918127087\n",
      "Epoch: 288\n",
      "\tTraining loss: 2.1471299354131994\n",
      "Epoch: 289\n",
      "\tTraining loss: 2.359036416442294\n",
      "Epoch: 290\n",
      "\tTraining loss: 2.06298410258935\n",
      "Epoch: 291\n",
      "\tTraining loss: 2.259684645533343\n",
      "Epoch: 292\n",
      "\tTraining loss: 2.1458120668659753\n",
      "Epoch: 293\n",
      "\tTraining loss: 2.277466678241179\n",
      "Epoch: 294\n",
      "\tTraining loss: 2.204899706499773\n",
      "Epoch: 295\n",
      "\tTraining loss: 2.2371576671508246\n",
      "Epoch: 296\n",
      "\tTraining loss: 2.0937801374186984\n",
      "Epoch: 297\n",
      "\tTraining loss: 2.3012770726493423\n",
      "Epoch: 298\n",
      "\tTraining loss: 2.2440651910480605\n",
      "Epoch: 299\n",
      "\tTraining loss: 2.214733661559743\n",
      "Epoch: 300\n",
      "\tTraining loss: 2.18788468594717\n",
      "\t\tTrain mse: 2.078\n",
      "Evaluating NeuralNetworkRegressor...\n",
      "\t\tValidation mse: 12.245\n",
      "Elapsed time: 0.7914 seconds\n",
      "\n",
      "Points Earned: 40.0\n",
      "==================================================\n",
      "Building model NeuralNetworkClassifier\n",
      "Skipping download. File already exists: /Users/hritz/Downloads/Neural Network Mini Project/datasets/data/MNIST/train.zip\n",
      "\n",
      "Unzipping: /Users/hritz/Downloads/Neural Network Mini Project/datasets/data/MNIST/train.zip\n",
      "\n",
      "Skipping download. File already exists: /Users/hritz/Downloads/Neural Network Mini Project/datasets/data/MNIST/val.zip\n",
      "\n",
      "Unzipping: /Users/hritz/Downloads/Neural Network Mini Project/datasets/data/MNIST/val.zip\n",
      "\n",
      "Loading dataset with Pandas...\n",
      "Done!\n",
      "Returning data as NumPy array...\n",
      "Training NeuralNetworkClassifier...\n",
      "\tUsing hyperparameters: \n",
      "\t\tneurons_per_layer = [128, 64, 10]\n",
      "\t\tlearning_curve_loss = nll\n",
      "\t\tdelta_loss_func = delta_softmax_nll\n",
      "\t\tg_hidden = ReLU\n",
      "\t\tg_output = Softmax\n",
      "\t\talpha = 0.01\n",
      "\t\tepochs = 10\n",
      "\t\tbatch_size = 64\n",
      "\t\tverbose = True\n",
      "\t\tseed = 42\n",
      "Epoch: 1\n",
      "\tTraining loss: 1.029514403452759\n",
      "Epoch: 2\n",
      "\tTraining loss: 0.41089863410851746\n",
      "Epoch: 3\n",
      "\tTraining loss: 0.3333281968350777\n",
      "Epoch: 4\n",
      "\tTraining loss: 0.29779153330622427\n",
      "Epoch: 5\n",
      "\tTraining loss: 0.2731494478598492\n",
      "Epoch: 6\n",
      "\tTraining loss: 0.25414291397040734\n",
      "Epoch: 7\n",
      "\tTraining loss: 0.23825419011560747\n",
      "Epoch: 8\n",
      "\tTraining loss: 0.2239321618911458\n",
      "Epoch: 9\n",
      "\tTraining loss: 0.21164650379694097\n",
      "Epoch: 10\n",
      "\tTraining loss: 0.20061667024236832\n",
      "\t\tTrain acc: 0.944\n",
      "Evaluating NeuralNetworkClassifier...\n",
      "\t\tValidation acc: 0.938\n",
      "Elapsed time: 35.9204 seconds\n",
      "\n",
      "Points Earned: 40.0\n",
      "==================================================\n",
      "\n",
      "Elapsed time: 36.7119 seconds\n",
      "Tests passed: 2/2, Total Points: 80/80\n",
      "\n",
      "Final Validation MSE: 12.244970073138418\n",
      "Final Validation Accuracy: 0.9383035714285715\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681894f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c913ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
