{"cells":[{"cell_type":"markdown","source":["# [Capstone Project] ML for Healthcare\n","\n","\n"],"metadata":{"id":"MBIPlgwErdKj"},"id":"MBIPlgwErdKj"},{"cell_type":"markdown","source":["## Introduction\n","\n","You are supposed to program in Python to analyze a real-world dataset.\n","\n","### Rule to complete the assignment\n","The basic rule is to write down your code after Each **TODO** line. There's **no limitation** on what libraries you use, how many lines code you write, how many cells you use in JupyterNotebook, etc.\n","\n","### Grading\n","The grade will be given based on the performance of your model on a testing dataset. Note, this **testing dataset** is private to the instructor and TA. As a student, you don't have access to the test dataset. What you can do is to refine or polish your model based on the **training** and **validation** set. Submit the best model you have."],"metadata":{"id":"a96DB6WusScU"},"id":"a96DB6WusScU"},{"cell_type":"markdown","id":"fd8c2c3b","metadata":{"id":"fd8c2c3b"},"source":["# Overview\n","We will delve into the practical application of Artificial Intelligence within the field of healthcare.\n","This lab will include:\n","- Dataset introduction\n","- Preprocessing\n","- Problem definition\n","- Feature engineering\n","- Model selection\n","- Training the model\n","- Validation and hyperparameter tuning\n","- Evaluation metrics\n"]},{"cell_type":"markdown","id":"a6e0d170","metadata":{"id":"a6e0d170"},"source":["## Dataset introduction\n","For this lab, we will take one ECG dataset as example from the UCR Time Series Classification Archive.\n","\n","### **UCR Time Series Classification Archive**  \n","Maintained by the University of California, Riverside (UCR), it is a valuable resource and repository for time series data and related classification problems. This archive is a comprehensive collection of various time series datasets designed specifically for benchmarking and evaluating time series classification algorithms and methods.\n","FYI, the whole UCR raw data download link: https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/\n","\n","But in this lab, you don't need it. **Please download the ECG5000 dataset on Canvas under the module of [Lab] Applied AI in Healthcare.**"]},{"cell_type":"markdown","id":"1749d9ac","metadata":{"id":"1749d9ac"},"source":["### **ECG5000**  \n","> The original data set for *ECG5000* is a 20-hour long ECG downloaded from Physionet_ATM [1]. The database is BIDMC Congestive Heart Failure Database(chfdb) and the record is chf07. It was originally published by Goldberger et al. [2]. The data were pre-processed in two steps, first extracting each heartbeat and then making each heartbeat equal length using interpolation. These data were originally used by Chen et al. [3]. After that, 5000 heartbeats were randomly selected to make the current data set. Data were from a patient who has severe congestive heart failure. The class values were obtained by automated annotation.\n","\n",">[1] https://physionet.org/cgi-bin/atm/ATM  \n",">[2] Goldberger, Ary L., et al. \"PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals.\" Circulation 101.23 (2000): e215-e220.  \n",">[3] Chen, Yanping, et al. \"A general framework for never-ending learning from time series streams.\" Data Mining and Knowledge Discovery 29.6 (2015): 1622-1664.  \n",">[4] http://www.timeseriesclassification.com/description.php?Dataset=ECG5000"]},{"cell_type":"markdown","id":"c84952b8","metadata":{"id":"c84952b8"},"source":["|                 Class name                 | Abbreviation |\n","|:------------------------------------------:|:------------:|\n","|                   Normal                   |       N      |            \n","|  R-on-T premature ventricular contraction  |       r      |             \n","| Supraventricular premature or ectopic beat |       S      |           \n","|      Premature ventricular contraction     |       V      |            \n","|             Unclassifiable beat            |       Q      |      "]},{"cell_type":"markdown","source":["## TODO 1 Load the dataset (2 points)\n","\n","0. Download the ECG5000 dataset on Canvas under the module of [Lab] Applied AI in Healthcare. Put the folder at the same path as your Jupyter code (aka, this file). In other words, th path should be *ECG5000/ECG5000_train.pickle* when loading the data in this file.\n","\n","After my modification, the dataset contains 5000 samples, 500 in training, 1500 in validation, and 3000 for testing. **However, the testing set is unaviliable to you!** The testing set is used to evaluate your model and grading.\n","\n","Your task is to train your model on the training set and evaluate your model on the validation set.\n","\n","1. Load the *ECG5000_train.pickle* and *ECG5000_validation.pickle* file, save the file into variable *ECGdataset*.\n","\n","2. Print out the first Sample, only the first sample.\n","\n","3. What the value of the first feature of the 10-th sample? Print it out.\n","\n","**Note:** The time series length of ECG5000 is 140 which is the feature, the labels is the first column of the dataset array.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ina0xQnkr9Um"},"id":"ina0xQnkr9Um"},{"cell_type":"code","execution_count":null,"id":"0a9a2b9a","metadata":{"id":"0a9a2b9a"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## TODO 2 Preprocessing (4 points)\n","\n","1. Normalize each sample (features) following the time-dimention, using the *sklearn.preprocessing.StandardScaler* function. Please remember to install and import the *sklearn* library.\n","\n","2. Note, as the training and validation sets are already splitted into two different files, you need to do the preprocessing for them independently (seperately)."],"metadata":{"id":"8ybNyRGN8WhB"},"id":"8ybNyRGN8WhB"},{"cell_type":"code","execution_count":null,"id":"52fa4a22","metadata":{"id":"52fa4a22"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"xeUlQkDGU8NI"},"id":"xeUlQkDGU8NI","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TODO 3 Data Splitting (Skip in this lab)\n","\n","1. Generally, you need to randomly split the dataset into two parts and name them as *Training_set* and *Validation_set* .\n","\n","However, as I already split the data for you. **You can skip this TODO**."],"metadata":{"id":"TsbfP8UX8Xwz"},"id":"TsbfP8UX8Xwz"},{"cell_type":"markdown","source":["## TODO 4 Train a KNN classifier (4 points)\n","\n","1. Use Sklearn library, train a KNN classifier (set K equals to *5*), feed the *Training_set* to the classifier."],"metadata":{"id":"_xC5m2BY8Y-r"},"id":"_xC5m2BY8Y-r"},{"cell_type":"code","source":[],"metadata":{"id":"LMJfhjz28ZLI"},"id":"LMJfhjz28ZLI","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TODO 5 Validate the classifier (8 points)\n","\n","\n","1. Use the trained classifier to predict the classes of the *Validation_set* .\n","\n","2. Print out the Accuracy, Precision, Recall, F1 score, AUROC, confusion matrix. For the matrix (except confusion matrix),keep 4 digits (such as 0.9856).\n","\n","3. Create a list, named *model_performance_knn* with 5 elements, which are Accuracy, Precision, Recall, F1 score, AUROC, respectively. Create a numpy array named *cf_matrix* to store the confusion matrix.\n","\n"],"metadata":{"id":"54xLvhSl8ZRn"},"id":"54xLvhSl8ZRn"},{"cell_type":"code","source":[],"metadata":{"id":"y0hMpWV68ZXX"},"id":"y0hMpWV68ZXX","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TODO 6 Visulize the confusion matrix (2 points)\n","\n","1. Use *seaborn* library to visualize the confusion matrix using *heatmap*. The labels should contain the counts and percentage.\n","\n","2. FYI, you may check how to use seaborn to draw heatmap at https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea"],"metadata":{"id":"bhrwzFxr8ZdQ"},"id":"bhrwzFxr8ZdQ"},{"cell_type":"code","execution_count":null,"id":"c54607fd","metadata":{"id":"c54607fd"},"outputs":[],"source":["import seaborn as sns\n","\n","plt.figure(dpi=100)\n","\n","group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n","group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n","labels = [f\"{v1}\\n{v2}\" for v1, v2, in zip(group_counts,group_percentages)]\n","labels = np.asarray(labels).reshape(5,5)\n","sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n","\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}