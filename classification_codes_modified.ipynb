{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#**ITCS4156-Introduction to Machine Lerning**\n",
        "\n",
        "#**Classification Assignment**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your Name: **Hritika Kucheriya**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install wget\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import traceback\n",
        "from pdb import set_trace\n",
        "import sys\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "import traceback\n",
        "from scipy.stats import norm  # Add missing import for norm distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from util.timer import Timer\n",
        "from util.data import split_data, dataframe_to_array, binarize_classes\n",
        "from util.metrics import accuracy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from datasets.MNISTDataset import MNISTDataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understand MNIST Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://camo.githubusercontent.com/01c057a753e92a9bc70b8c45d62b295431851c09cffadf53106fc0aea7e2843f/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687964656661756c742e6a7067)\n",
        "\n",
        "The dataset you'll be using for this project is the famous [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset which contains images of handwritten digits 0 through 9. There are 60,000 images included in the dataset and each image is a gray scale image of size 28x28. Each pixel represents a feature which means there are $28*28$ or $784$ features per each data sample.\n",
        "\n",
        "**The goal of the dataset is to classify each image of handwritten digits correctly!**\n",
        "\n",
        "The dataset consists of 3 splits:\n",
        "\n",
        "1. **Train**: Throughout this assignment you will be training your model using this data. There are approximately 44k training samples.\n",
        "2. **Validation**: You will then use this set to tune your model and evaluate its performance. There are approximately 12k training samples.\n",
        "3. **Test**: This split simulates real life data which we often don't have access to until the model is deployed. We have kept this split hidden from you and we will use it to judge the performance of your model on Autolab.\n",
        "\n",
        "You DO NOT have access to the Test set as it gonna be used for scoring. This will not prevent you to complete this assignment at all.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Design Machine Learning Models (TODO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Model\n",
        "Basic model structure, **don't change** this component.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseModel(ABC):\n",
        "    \"\"\" Super class for ITCS Machine Learning Class\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit(self, X, y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ClassificationModel \n",
        "Basic model structure, **don't change** this component.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassificationModel(BaseModel):\n",
        "    \"\"\"\n",
        "        Abstract class for classification \n",
        "        \n",
        "        Attributes\n",
        "        ==========\n",
        "    \"\"\"\n",
        "\n",
        "    # check if the matrix is 2-dimensional. if not, raise an exception    \n",
        "    def _check_matrix(self, mat, name):\n",
        "        if len(mat.shape) != 2:\n",
        "            raise ValueError(f\"Your matrix {name} shape is not 2D! Matrix {name} has the shape {mat.shape}\")\n",
        "        \n",
        "    ####################################################\n",
        "    #### abstract funcitons ############################\n",
        "    @abstractmethod\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"\n",
        "            train classification model\n",
        "            \n",
        "            Args:\n",
        "                X:  Input data\n",
        "                \n",
        "                y:  targets/labels\n",
        "        \"\"\"        \n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def predict(self, X: np.ndarray):\n",
        "        \"\"\"\n",
        "            apply the learned model to input X\n",
        "            \n",
        "            parameters\n",
        "            ----------\n",
        "            X     2d array\n",
        "                  input data\n",
        "            \n",
        "        \"\"\"        \n",
        "        pass \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO: Classification with Perceptron\n",
        "*Please complete the TODOs. *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Perceptron(ClassificationModel):\n",
        "    \"\"\"\n",
        "        Performs Gaussian Naive Bayes\n",
        "    \n",
        "        attributes:\n",
        "            alpha: learning rate or step size used by gradient descent.\n",
        "                \n",
        "            epochs (int): Number of times data is used to update the weights `self.w`.\n",
        "                Each epoch means a data sample was used to update the weights at least\n",
        "                once.\n",
        "                \n",
        "            seed (int): Seed to be used for NumPy's RandomState class\n",
        "                or universal seed np.random.seed() function.\n",
        "            \n",
        "            batch_size (int): Mini-batch size used to determine the size of mini-batches\n",
        "                if mini-batch gradient descent is used.\n",
        "            \n",
        "            w (np.ndarray): NumPy array which stores the learned weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha: float, epochs: int = 1, seed: int = None):\n",
        "        ClassificationModel.__init__(self)\n",
        "        self.alpha = alpha\n",
        "        self.epochs = epochs\n",
        "        self.seed = seed\n",
        "        self.w = None\n",
        "        \n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\" Train model to learn optimal weights when performing binary classification.\n",
        "        \n",
        "            Args:\n",
        "                X: Data \n",
        "                \n",
        "                y: Targets/labels\n",
        "                \n",
        "             TODO:\n",
        "                Finish this method by using Rosenblatt's Perceptron algorithm to learn\n",
        "                the best weights to classify the binary data. There is no need to\n",
        "                implement th pocket algorithm unless you choose to do so. Also, update \n",
        "                and store the learned weights into `self.w`.\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.seed is not None:\n",
        "             np.random.seed(self.seed)\n",
        "        \n",
        "        # Preprocess data - add bias term\n",
        "        X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        \n",
        "        # Better weight initialization\n",
        "        self.w = np.zeros((X_bias.shape[1], 1))\n",
        "        \n",
        "        # Format labels to -1 and 1 for easier computation\n",
        "        y_formatted = np.where(y == 0, -1, 1).reshape(-1, 1)\n",
        "        \n",
        "        # Implement averaged perceptron\n",
        "        avg_w = np.zeros_like(self.w)\n",
        "        best_w = None\n",
        "        best_accuracy = 0\n",
        "        \n",
        "        # Initial learning rate\n",
        "        alpha = self.alpha\n",
        "        \n",
        "        # Counter for averaging\n",
        "        c = 1\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            # Learning rate decay with a slower decay rate\n",
        "            current_alpha = alpha / (1 + 0.001 * epoch)\n",
        "            \n",
        "            # Shuffle the data for each epoch\n",
        "            indices = np.random.permutation(X_bias.shape[0])\n",
        "            X_shuffled = X_bias[indices]\n",
        "            y_shuffled = y_formatted[indices]\n",
        "            \n",
        "            errors = 0\n",
        "            \n",
        "            for i in range(X_shuffled.shape[0]):\n",
        "                # Calculate net input\n",
        "                net_input = np.dot(X_shuffled[i], self.w)\n",
        "                \n",
        "                # Check if prediction is correct\n",
        "                if y_shuffled[i] * net_input <= 0:\n",
        "                    # Update weights\n",
        "                    update = current_alpha * y_shuffled[i] * X_shuffled[i].reshape(-1, 1)\n",
        "                    self.w += update\n",
        "                    errors += 1\n",
        "                \n",
        "                # Update average weights\n",
        "                avg_w += self.w\n",
        "                c += 1\n",
        "            \n",
        "            # Early stopping if no errors\n",
        "            if errors == 0 and epoch > 10:\n",
        "                break\n",
        "            \n",
        "            # Evaluate current model\n",
        "            predictions = np.sign(np.dot(X_bias, self.w))\n",
        "            predictions[predictions <= 0] = 0  # Convert -1 to 0\n",
        "            current_accuracy = np.mean(predictions == y)\n",
        "            \n",
        "            # Save best model\n",
        "            if current_accuracy > best_accuracy:\n",
        "                best_accuracy = current_accuracy\n",
        "                best_w = self.w.copy()\n",
        "        \n",
        "        # Use averaged weights or best weights, whichever is better\n",
        "        avg_w = avg_w / c\n",
        "        \n",
        "        # Evaluate averaged model\n",
        "        avg_predictions = np.sign(np.dot(X_bias, avg_w))\n",
        "        avg_predictions[avg_predictions <= 0] = 0  # Convert -1 to 0\n",
        "        avg_accuracy = np.mean(avg_predictions == y)\n",
        "        \n",
        "        # Choose the better model\n",
        "        if avg_accuracy > best_accuracy:\n",
        "            self.w = avg_w\n",
        "        else:\n",
        "            self.w = best_w\n",
        "\n",
        "    def predict(self, X: np.ndarray):\n",
        "        \"\"\" Make predictions using the learned weights.\n",
        "        \n",
        "            Args:\n",
        "                X: Data \n",
        "\n",
        "            TODO:\n",
        "                Finish this method by adding code to make a prediction given the learned\n",
        "                weights `self.w`. Store the predicted labels into `y_hat`.\n",
        "        \"\"\"\n",
        "        # TODO Add code below\n",
        "        \n",
        "        X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        \n",
        "        net_input = np.dot(X_bias, self.w)\n",
        "        \n",
        "        \n",
        "        y_hat = np.ones([len(X), 1]) # TODO Store predictions here by replacing np.ones()\n",
        "        # Makes sure predictions are given as a 2D array\n",
        "\n",
        "        y_hat = np.where(net_input >= 0, 1, 0)\n",
        "        return y_hat.reshape(-1, 1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO: Classification with NaiveBayes\n",
        "*Please complete the TODOs. *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NaiveBayes(ClassificationModel):\n",
        "    \"\"\"\n",
        "        Performs Gaussian Naive Bayes\n",
        "    \n",
        "        attributes:\n",
        "            smoothing: smoothing hyperparameter used to prevent numerical instability and \n",
        "                divide by zero errors\n",
        "                \n",
        "            class_labels (np.ndarray or list): Unique labels for the passed data. This \n",
        "                should be set in the fit() method.\n",
        "            \n",
        "            priors (np.ndarray): NumPy array which stores the priors.\n",
        "            \n",
        "            log_priors (np.ndarray): NumPy array which stores the log of the priors and\n",
        "                used by the predict() method.\n",
        "            \n",
        "            means (np.ndarray): NumPy array of means used by the\n",
        "                log_gaussian_distribution() method to compute the log likelihoods\n",
        "            \n",
        "            stds (np.ndarray): NumPy array of standard deviations used by the\n",
        "                log_gaussian_distribution() method to compute the log likelihoods\n",
        "            \n",
        "            feature_importance (np.ndarray): NumPy array of feature importance scores\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothing: float = 10e-3):\n",
        "        ClassificationModel.__init__(self)\n",
        "        self.smoothing = smoothing\n",
        "        # All class variables that need to be set somewhere within the below methods.\n",
        "        self.class_labels = None\n",
        "        self.priors = None\n",
        "        self.log_priors = None\n",
        "        self.means = None\n",
        "        self.stds = None\n",
        "        self.feature_importance = None  # Store feature importance scores\n",
        "\n",
        "    def log_gaussian_distribution(self, X: np.ndarray, mu: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "        \"\"\" Computes the log of a value at a given point in a Gaussian distribution\n",
        "        \n",
        "            Args:\n",
        "                X: Data for which an output value is computed for.\n",
        "                \n",
        "                mu: Feature means\n",
        "                \n",
        "                var: Feature variance\n",
        "        \"\"\"\n",
        "        # Improved numerical stability\n",
        "        variance = std**2\n",
        "        # Avoid very small variances that could cause numerical issues\n",
        "        variance = np.maximum(variance, 1e-10)\n",
        "        return -0.5 * np.log(2 * np.pi * variance) - 0.5 * ((X - mu)**2) / variance\n",
        "        \n",
        "    def compute_priors(self, y: np.ndarray) -> None:\n",
        "        \"\"\" Computes the priors and log priors for each class.\n",
        "    \n",
        "            Args:\n",
        "                y: Lables\n",
        "                \n",
        "            TODO: \n",
        "                Finish this method by computing the priors and log priors to be used when\n",
        "                making predictions using MAP. Store the computed priors and log priors \n",
        "                into self.priors and self.log_priors.\n",
        "                \n",
        "        \"\"\"\n",
        "        # TODO Add code below\n",
        "        \n",
        "        self.class_labels = np.unique(y)\n",
        "        \n",
        "        \n",
        "        class_counts = np.zeros(len(self.class_labels))\n",
        "        for i, label in enumerate(self.class_labels):\n",
        "            class_counts[i] = np.sum(y == label)\n",
        "        \n",
        "        # Apply Laplace smoothing to the priors for better balance\n",
        "        class_counts += 1  # Add 1 to each class count (Laplace smoothing)\n",
        "        \n",
        "        # Compute priors with smoothing\n",
        "        self.priors = class_counts / np.sum(class_counts)\n",
        "        self.log_priors = np.log(self.priors)\n",
        "\n",
        "    def compute_parameters(self, X: np.ndarray, y: np.ndarray) -> None:\n",
        "        \"\"\" Computes the means and standard deviations for classes and features\n",
        "        \n",
        "            Args:\n",
        "                X: Data \n",
        "                \n",
        "                y: Targets/labels\n",
        "\n",
        "            TODO: \n",
        "                Finish this method by computing the means and stds for the Gaussian\n",
        "                distribution which will then be used to comput the likelihoods. Store\n",
        "                the computed means and stds into self.means and self.stds.\n",
        "        \"\"\"\n",
        "        # TODO Add code below\n",
        "        \n",
        "        n_features = X.shape[1]\n",
        "        n_classes = len(self.class_labels)\n",
        "        \n",
        "        self.means = np.zeros((n_classes, n_features))\n",
        "        self.stds = np.zeros((n_classes, n_features))\n",
        "        \n",
        "        # Ensure y is flattened to avoid shape mismatch\n",
        "        y = y.flatten()\n",
        "        \n",
        "        # Calculate feature importance based on variance between classes\n",
        "        self.feature_importance = np.zeros(n_features)\n",
        "        \n",
        "        # First compute means for each class\n",
        "        class_means = np.zeros((n_classes, n_features))\n",
        "        for i, label in enumerate(self.class_labels):\n",
        "            class_samples = X[y == label]\n",
        "            if len(class_samples) > 0:\n",
        "                class_means[i] = np.mean(class_samples, axis=0)\n",
        "        \n",
        "        # Compute feature importance as the variance of means between classes\n",
        "        # Features with higher variance between class means are more discriminative\n",
        "        for j in range(n_features):\n",
        "            self.feature_importance[j] = np.var(class_means[:, j])\n",
        "        \n",
        "        # Normalize feature importance\n",
        "        if np.sum(self.feature_importance) > 0:\n",
        "            self.feature_importance = self.feature_importance / np.sum(self.feature_importance)\n",
        "        \n",
        "        for i, label in enumerate(self.class_labels):\n",
        "            # Get samples for this class\n",
        "            class_samples = X[y == label]\n",
        "            \n",
        "            if len(class_samples) > 0:\n",
        "                # Compute mean and std for each feature\n",
        "                self.means[i] = np.mean(class_samples, axis=0)\n",
        "                # Use a more robust std calculation\n",
        "                self.stds[i] = np.std(class_samples, axis=0) + self.smoothing\n",
        "            else:\n",
        "                # Handle the case where a class has no samples\n",
        "                self.means[i] = np.zeros(n_features)\n",
        "                self.stds[i] = np.ones(n_features) * self.smoothing\n",
        "\n",
        "    def compute_log_likelihoods(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\" Computes and returns log likelihoods using the means and stds\n",
        "                \n",
        "            Args:\n",
        "                X: Data \n",
        "        \n",
        "            TODO:\n",
        "                Finish this method by computing the log likelihoods of the passed data \n",
        "                `X`. Use the `self.means` and `self.stds` class variables you set in \n",
        "                the `compute_parameters()` method along with the `log_gaussian_distribution()` \n",
        "                method which is defined for you. The `log_gaussian_distribution()`  \n",
        "                will apply the log to your feature likelihoods for you so you don't need to!\n",
        "                This method should return the computed log likelihoods.\n",
        "        \"\"\"\n",
        "        \n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.class_labels)\n",
        "        log_likelihoods = np.zeros((n_samples, n_classes))\n",
        "        \n",
        "        # Optimized implementation with feature importance weighting\n",
        "        for i in range(n_classes):\n",
        "            # Pre-compute log-likelihoods for each feature and class\n",
        "            feature_log_likelihoods = np.zeros((n_samples, X.shape[1]))\n",
        "            \n",
        "            for j in range(X.shape[1]):\n",
        "                # Weight the log-likelihood by feature importance\n",
        "                feature_weight = 1.0\n",
        "                if self.feature_importance is not None:\n",
        "                    feature_weight = 1.0 + 9.0 * self.feature_importance[j]  # Scale between 1 and 10\n",
        "                \n",
        "                feature_log_likelihoods[:, j] = feature_weight * self.log_gaussian_distribution(\n",
        "                    X[:, j], self.means[i, j], self.stds[i, j]\n",
        "                )\n",
        "            \n",
        "            # Sum log-likelihoods across features for each sample\n",
        "            log_likelihoods[:, i] = np.sum(feature_log_likelihoods, axis=1)\n",
        "        \n",
        "        return log_likelihoods\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
        "        \"\"\" Computes the priors and Gaussian parameters used for predicting.\n",
        "        \n",
        "            Args:\n",
        "                X: Data \n",
        "                \n",
        "                y: Targets/labels\n",
        "                \n",
        "             TODO:\n",
        "                Finish this method by computing the priors and Gaussian parameters. \n",
        "                To do so, first finish and then call the compute_parameters() and \n",
        "                compute_priors() methods.\n",
        "        \"\"\"\n",
        "        self.class_labels = np.unique(y)\n",
        "        # TODO Add code below\n",
        "        \n",
        "        self.compute_parameters(X, y)\n",
        "        self.compute_priors(y)\n",
        "\n",
        "    def predict(self, X) -> np.ndarray:\n",
        "        \"\"\" Comptues a prediction using log likelihoods and log priors.\n",
        "        \n",
        "            Args:\n",
        "                X: Data \n",
        "        \n",
        "             TODO:\n",
        "                Finish this method by computing the log likelihoods and log priors.\n",
        "                To do so, first finishing and then call the compute_log_likelihoods() \n",
        "                method. You'll also need to access the class variables self.log_priors \n",
        "                and self.class_labels you set when running the fit(), compute_parameters() \n",
        "                and compute_priors() methods. Store the predicted labels into `y_hat`.\n",
        "        \"\"\"\n",
        "        # TODO Add code below\n",
        "        \n",
        "        log_likelihoods = self.compute_log_likelihoods(X)\n",
        "        \n",
        "        posterior_log_probs = log_likelihoods + self.log_priors\n",
        "        \n",
        "        predicted_class_indices = np.argmax(posterior_log_probs, axis=1)\n",
        "        \n",
        "        # Map indices to actual class labels\n",
        "        y_hat = self.class_labels[predicted_class_indices]\n",
        "        \n",
        "        # Makes sure predictions are given as a 2D array\n",
        "        return y_hat.reshape(-1, 1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO: Classification with Logistic Regression\n",
        "*Please complete the TODOs. *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegression(ClassificationModel):\n",
        "    \"\"\"\n",
        "        Performs Logistic Regression using the softmax function.\n",
        "    \n",
        "        attributes:\n",
        "            alpha: learning rate or step size used by gradient descent.\n",
        "                \n",
        "            epochs: Number of times data is used to update the weights `self.w`.\n",
        "                Each epoch means a data sample was used to update the weights at least\n",
        "                once.\n",
        "            \n",
        "            seed (int): Seed to be used for NumPy's RandomState class\n",
        "                or universal seed np.random.seed() function.\n",
        "            \n",
        "            batch_size: Mini-batch size used to determine the size of mini-batches\n",
        "                if mini-batch gradient descent is used.\n",
        "            \n",
        "            w (np.ndarray): NumPy array which stores the learned weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha: float, epochs: int = 1,  seed: int = None, batch_size: int = None):\n",
        "        ClassificationModel.__init__(self)\n",
        "        self.alpha = alpha\n",
        "        self.epochs = epochs\n",
        "        self.seed = seed\n",
        "        self.batch_size = batch_size\n",
        "        self.w = None\n",
        "        self.reg_lambda = 0.01  # L2 regularization parameter\n",
        "\n",
        "    def softmax(self, z: np.ndarray) -> np.ndarray:\n",
        "        \"\"\" Computes probabilities for multi-class classification given continuous inputs z.\n",
        "        \n",
        "            Args:\n",
        "                z: Continuous outputs after dotting the data with the current weights \n",
        "\n",
        "            TODO:\n",
        "                Finish this method by adding code to return the softmax. Don't forget\n",
        "                to subtract the max from `z` to maintain  numerical stability!\n",
        "        \"\"\"\n",
        "        \n",
        "        # Improved numerical stability by subtracting max value\n",
        "        z_stable = z - np.max(z, axis=1, keepdims=True)\n",
        "        \n",
        "        # Clip values to prevent overflow/underflow\n",
        "        z_stable = np.clip(z_stable, -500, 500)\n",
        "        \n",
        "        exp_z = np.exp(z_stable)\n",
        "        \n",
        "        # Add small epsilon to denominator for numerical stability\n",
        "        return exp_z / (np.sum(exp_z, axis=1, keepdims=True) + 1e-10)\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
        "        \"\"\" Train our model to learn optimal weights for classifying data.\n",
        "        \n",
        "            Args:\n",
        "                X: Data \n",
        "                \n",
        "                y: Targets/labels\n",
        "                \n",
        "             TODO:\n",
        "                Finish this method by using either batch or mini-batch gradient descent\n",
        "                to learn the best weights to classify the data. You'll need to finish and \n",
        "                also call the `softmax()` method to complete this method. Also, update \n",
        "                and store the learned weights into `self.w`. \n",
        "        \"\"\"\n",
        "         # Initialize weights\n",
        "        if self.seed is not None:\n",
        "             np.random.seed(self.seed)\n",
        "         \n",
        "        \n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(np.unique(y))\n",
        "         \n",
        "        \n",
        "        self.w = np.random.randn(n_features, n_classes) * 0.01\n",
        "         \n",
        "        \n",
        "        y_one_hot = np.zeros((n_samples, n_classes))\n",
        "        for i in range(n_samples):\n",
        "             y_one_hot[i, int(y[i])] = 1\n",
        "         \n",
        "        # Learning rate schedule\n",
        "        initial_alpha = self.alpha\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            # Implement learning rate decay\n",
        "            current_alpha = initial_alpha / (1 + 0.01 * epoch)\n",
        "            \n",
        "            if self.batch_size is None:\n",
        "                \n",
        "                z = X @ self.w\n",
        "                y_hat = self.softmax(z)\n",
        "                \n",
        "                \n",
        "                # Add L2 regularization term to gradient\n",
        "                gradient = X.T @ (y_hat - y_one_hot) / n_samples + self.reg_lambda * self.w\n",
        "                \n",
        "               \n",
        "                self.w -= current_alpha * gradient\n",
        "            else:\n",
        "                \n",
        "                indices = np.random.permutation(n_samples)\n",
        "                for start_idx in range(0, n_samples, self.batch_size):\n",
        "                    batch_indices = indices[start_idx:start_idx + self.batch_size]\n",
        "                    X_batch = X[batch_indices]\n",
        "                    y_batch = y_one_hot[batch_indices]\n",
        "                    \n",
        "                    \n",
        "                    z_batch = X_batch @ self.w\n",
        "                    y_hat_batch = self.softmax(z_batch)\n",
        "                    \n",
        "                    \n",
        "                    # Add L2 regularization term to gradient\n",
        "                    gradient = X_batch.T @ (y_hat_batch - y_batch) / len(batch_indices) + self.reg_lambda * self.w\n",
        "                    \n",
        "                   \n",
        "                    self.w -= current_alpha * gradient\n",
        "\n",
        "       \n",
        "    def predict(self, X: np.ndarray):\n",
        "        \"\"\" Make predictions using the learned weights.\n",
        "        \n",
        "            Args:\n",
        "                X: Data \n",
        "\n",
        "            TODO:\n",
        "                Finish this method by adding code to make a prediction given the learned\n",
        "                weights `self.w`. Store the predicted labels into `y_hat`.\n",
        "        \"\"\"\n",
        "        # TODO Add code below\n",
        "\n",
        "        z = X @ self.w\n",
        "        \n",
        "        y_proba = self.softmax(z)\n",
        "        \n",
        "        # Get the class with the highest probability\n",
        "        y_hat = np.argmax(y_proba, axis=1)\n",
        "\n",
        "        # Makes sure predictions are given as a 2D array\n",
        "        return y_hat.reshape(-1, 1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO: Define Hyperparameters \n",
        "*Please complete the TODOs. *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class HyperParametersAndTransforms():\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_params(name):\n",
        "        model = getattr(HyperParametersAndTransforms, name)\n",
        "        params = {}\n",
        "        for key, value in model.__dict__.items():\n",
        "            if not key.startswith('__') and not callable(key):\n",
        "                if not callable(value) and not isinstance(value, staticmethod):\n",
        "                    params[key] = value\n",
        "        return params\n",
        "    \n",
        "    class Perceptron():\n",
        "        \"\"\"Kwargs for classifier the Perceptron class and data prep\"\"\"\n",
        "        model_kwargs = dict(\n",
        "            alpha = 1.0,  # Higher learning rate\n",
        "            epochs = 2000,  # More epochs for thorough training\n",
        "            seed = 42, # Setting seed for reproducible results\n",
        "        )\n",
        "\n",
        "        data_prep_kwargs = dict(\n",
        "            target_pipe = Pipeline([\n",
        "                ('identity', 'passthrough')\n",
        "            ]),\n",
        "            \n",
        "            feature_pipe = Pipeline([\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "        )\n",
        "        \n",
        "    class NaiveBayes():\n",
        "        \"\"\"Kwargs for classifier the NaiveBayes class and data prep\"\"\"\n",
        "        model_kwargs = dict(\n",
        "            smoothing = 5e-3,  # Increased smoothing parameter for better generalization\n",
        "        )\n",
        "        \n",
        "        data_prep_kwargs = dict(\n",
        "            target_pipe = Pipeline([\n",
        "                ('identity', 'passthrough')\n",
        "            ]),\n",
        "            feature_pipe = Pipeline([\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "        )\n",
        "        \n",
        "    class LogisticRegression():\n",
        "        model_kwargs = dict(\n",
        "            alpha = 0.1,    # Increased learning rate for faster convergence\n",
        "            epochs = 300,   # Increased number of training iterations for better convergence\n",
        "            seed = 42,      # Setting seed for reproducible results\n",
        "            batch_size = 256,  # Larger batch size for better gradient estimation\n",
        "        )\n",
        "      \n",
        "        data_prep_kwargs = dict(\n",
        "            target_pipe = Pipeline([\n",
        "                ('identity', 'passthrough')\n",
        "            ]),\n",
        "            \n",
        "            feature_pipe = Pipeline([\n",
        "                ('scaler', StandardScaler())\n",
        "            ])\n",
        "        )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training and Testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Preprocessing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DataPreparation():\n",
        "    def __init__(self, target_pipe, feature_pipe):\n",
        "        self.target_pipe = target_pipe\n",
        "        self.feature_pipe = feature_pipe\n",
        "        \n",
        "    @abstractmethod\n",
        "    def data_prep(self):\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        if self.target_pipe  is not None:\n",
        "            self.target_pipe.fit(y)\n",
        "            \n",
        "        if self.feature_pipe is not None:\n",
        "            self.feature_pipe.fit(X)\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        if self.target_pipe is not None:\n",
        "            y = self.target_pipe.transform(y)\n",
        "            \n",
        "        if self.feature_pipe is not None:\n",
        "            X = self.feature_pipe.transform(X)\n",
        "\n",
        "        return X, y\n",
        "    \n",
        "    def fit_transform(self, X, y):\n",
        "        self.fit(X, y)\n",
        "        X, y = self.transform(X, y)\n",
        "        return X, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MNISTDataPreparation(DataPreparation):\n",
        "    def __init__(self, target_pipe, feature_pipe):\n",
        "        super().__init__(target_pipe, feature_pipe)\n",
        "        \n",
        "    def data_prep(self, binarize=False, return_array=False):\n",
        "        mnist_dataset = MNISTDataset()\n",
        "        X_trn_df, y_trn_df, X_vld_df, y_vld_df = mnist_dataset.load()\n",
        "        \n",
        "        # Converts MNIST problem to classifying ONLY 1s vs 0s\n",
        "        if binarize:\n",
        "            X_trn_df, y_trn_df = binarize_classes(\n",
        "                X_trn_df, \n",
        "                y_trn_df, \n",
        "                pos_class=[1],\n",
        "                neg_class=[0], \n",
        "            )\n",
        "            \n",
        "            X_vld_df, y_vld_df = binarize_classes(\n",
        "                X_vld_df, \n",
        "                y_vld_df, \n",
        "                pos_class=[1], \n",
        "                neg_class=[0], \n",
        "            )\n",
        "\n",
        "        X_trn_df, y_trn_df = self.fit_transform(X=X_trn_df, y=y_trn_df)\n",
        "        X_vld_df, y_vld_df = self.transform(X=X_vld_df, y=y_vld_df)\n",
        "\n",
        "        if return_array:\n",
        "            print(\"Returning data as NumPy array...\")\n",
        "            return dataframe_to_array([X_trn_df, y_trn_df, X_vld_df, y_vld_df])\n",
        "            \n",
        "        return X_trn_df, y_trn_df, X_vld_df, y_vld_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Model running (training/fit and testing/evaluate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_name(obj):\n",
        "    try:\n",
        "        if hasattr(obj, '__name__'):\n",
        "            return obj.__name__\n",
        "        else:\n",
        "            return obj\n",
        "    except Exception as e:\n",
        "        return obj\n",
        "    \n",
        "def catch_and_throw(e, err):\n",
        "    trace = traceback.format_exc()\n",
        "    print(err + f\"\\n{trace}\")\n",
        "    raise e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RunModel():\n",
        "    t1 = '\\t'\n",
        "    t2 = '\\t\\t'\n",
        "    t3 = '\\t\\t\\t'\n",
        "    def __init__(self, model, model_params):\n",
        "        self.model_name = model.__name__\n",
        "        self.model_params = model_params\n",
        "        self.model = self.build_model(model, model_params)\n",
        "\n",
        "    def build_model(self, model, model_params):\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Building model {self.model_name}\")\n",
        "        \n",
        "        try:\n",
        "            model = model(**model_params)\n",
        "        except Exception as e:\n",
        "            err = f\"Exception caught while building model for {self.model_name}:\"\n",
        "            catch_and_throw(e, err)\n",
        "        return model\n",
        "    \n",
        "    def fit(self, *args, **kwargs):\n",
        "        print(f\"Training {self.model_name}...\")\n",
        "        print(f\"{self.t1}Using hyperparameters: \")\n",
        "        [print(f\"{self.t2}{n} = {get_name(v)}\")for n, v in self.model_params.items()]\n",
        "        try: \n",
        "            return self._fit(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            err = f\"Exception caught while training model for {self.model_name}:\"\n",
        "            catch_and_throw(e, err)\n",
        "            \n",
        "    def _fit(self, X, y, metrics=None, pass_y=False):\n",
        "        if pass_y:\n",
        "            self.model.fit(X, y)\n",
        "        else:\n",
        "             self.model.fit(X)\n",
        "        preds = self.model.predict(X)\n",
        "        scores = self.get_metrics(y, preds, metrics, prefix='Train')\n",
        "        return scores\n",
        "    \n",
        "    def evaluate(self, *args, **kwargs):\n",
        "        print(f\"Evaluating {self.model_name}...\")\n",
        "        try:\n",
        "            return self._evaluate(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            err = f\"Exception caught while evaluating model for {self.model_name}:\"\n",
        "            catch_and_throw(e, err)\n",
        "        \n",
        "\n",
        "    def _evaluate(self, X, y, metrics, prefix=''):\n",
        "        preds = self.model.predict(X)\n",
        "        scores = self.get_metrics(y, preds, metrics, prefix)      \n",
        "        return scores\n",
        "    \n",
        "    def predict(self, X):\n",
        "        try:\n",
        "            preds = self.model.predict(X)\n",
        "        except Exception as e:\n",
        "            err = f\"Exception caught while making predictions for model {self.model_name}:\"\n",
        "            catch_and_throw(e, err)\n",
        "            \n",
        "        return preds\n",
        "    \n",
        "    def get_metrics(self, y, y_hat, metrics, prefix=''):\n",
        "        scores = {}\n",
        "        for name, metric in metrics.items():\n",
        "            score = metric(y, y_hat)\n",
        "            display_score = round(score, 3)\n",
        "            scores[name] = score\n",
        "            print(f\"{self.t2}{prefix} {name}: {display_score}\")\n",
        "        return scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_eval(eval_stage='validation'):\n",
        "    main_timer = Timer()\n",
        "    main_timer.start()\n",
        "\n",
        "    total_points = 0\n",
        "    \n",
        "    task_info = [\n",
        "       dict(\n",
        "            model=Perceptron,\n",
        "            name='Perceptron',\n",
        "            data=MNISTDataPreparation,\n",
        "            data_prep=dict(binarize=True, return_array=True),\n",
        "            metrics=dict(acc=accuracy),\n",
        "            eval_metric='acc',\n",
        "            rubric=rubric_perceptron,\n",
        "            trn_score=0,\n",
        "            eval_score=0,\n",
        "            successful=False,\n",
        "        ),\n",
        "        dict(\n",
        "            model=NaiveBayes,\n",
        "            name='NaiveBayes',\n",
        "            data=MNISTDataPreparation,\n",
        "            data_prep=dict(return_array=True),\n",
        "            metrics=dict(acc=accuracy),\n",
        "            eval_metric='acc',\n",
        "            rubric=rubric_naive_bayes,\n",
        "            trn_score=0,\n",
        "            eval_score=0,\n",
        "            successful=False,\n",
        "        ),\n",
        "        dict(\n",
        "            model=LogisticRegression,\n",
        "            name='LogisticRegression',\n",
        "            data=MNISTDataPreparation,\n",
        "            data_prep=dict(return_array=True),\n",
        "            metrics=dict(acc=accuracy),\n",
        "            rubric=rubric_logistic_regression,\n",
        "            eval_metric='acc',\n",
        "            trn_score=0,\n",
        "            eval_score=0,\n",
        "            successful=False,\n",
        "        ),\n",
        "    ]\n",
        "    \n",
        "    total_points = 0\n",
        "    \n",
        "    for info in task_info:\n",
        "        task_timer =  Timer()\n",
        "        task_timer.start()\n",
        "        try:\n",
        "            params = HyperParametersAndTransforms.get_params(info['name'])\n",
        "            model_kwargs = params.get('model_kwargs', {})\n",
        "            data_prep_kwargs = params.get('data_prep_kwargs', {})\n",
        "            \n",
        "            run_model = RunModel(info['model'], model_kwargs)\n",
        "            data = info['data'](**data_prep_kwargs)\n",
        "            X_trn, y_trn, X_vld, y_vld = data.data_prep(**info['data_prep'])\n",
        "            \n",
        "            trn_scores = run_model.fit(X_trn, y_trn, info['metrics'], pass_y=True)\n",
        "            eval_scores = run_model.evaluate(X_vld, y_vld, info['metrics'], prefix=eval_stage.capitalize())\n",
        "            \n",
        "            info['trn_score'] = trn_scores[info['eval_metric']]\n",
        "            info['eval_score'] = eval_scores[info['eval_metric']]\n",
        "            info['successful'] = True\n",
        "                \n",
        "        except Exception as e:\n",
        "            track = traceback.format_exc()\n",
        "            print(\"The following exception occurred while executing this test case:\\n\", track)\n",
        "        task_timer.stop()\n",
        "        \n",
        "        print(\"\")\n",
        "        points = info['rubric'](info['eval_score'])\n",
        "        print(f\"Points Earned: {points}\")\n",
        "        total_points += points\n",
        "        \n",
        "    print(\"=\"*50)\n",
        "    print('')\n",
        "    main_timer.stop()\n",
        "    \n",
        "    avg_trn_acc, avg_eval_acc, successful_tests = summary(task_info)\n",
        "    task_eval_acc = get_eval_scores(task_info)\n",
        "    total_points = int(round(total_points))\n",
        "    \n",
        "    print(f\"Tests passed: {successful_tests}/{ len(task_info)}, Total Points: {total_points}/80\\n\")\n",
        "    print(f\"Average Train Accuracy: {avg_trn_acc}\")\n",
        "    print(f\"Average {eval_stage.capitalize()} Accuracy: {avg_eval_acc}\")\n",
        "    \n",
        "    return (total_points, avg_eval_acc, main_timer.last_elapsed_time, avg_trn_acc, *task_eval_acc)\n",
        "\n",
        "def summary(task_info):\n",
        "    sum_trn_acc = 0\n",
        "    sum_eval_acc = 0\n",
        "    successful_tests = 0\n",
        "\n",
        "    for info in task_info:\n",
        "        if info['successful']:\n",
        "            successful_tests += 1\n",
        "            sum_trn_acc += info['trn_score']\n",
        "            sum_eval_acc += info['eval_score']\n",
        "    \n",
        "    if successful_tests == 0:\n",
        "        return 0, 0, successful_tests\n",
        "    \n",
        "    avg_trn_acc = sum_trn_acc / len(task_info)\n",
        "    avg_eval_acc = sum_eval_acc / len(task_info)\n",
        "    return round(avg_trn_acc, 4), round(avg_eval_acc, 4), successful_tests\n",
        "\n",
        "def get_eval_scores(task_info):\n",
        "    return [i['eval_score'] for i in task_info]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Related Functions\n",
        "Don't change this section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rubric_perceptron(acc, max_score=25):\n",
        "    score_percent = 0\n",
        "    if acc >= 0.95:\n",
        "        score_percent = 100\n",
        "    elif acc >= 0.90:\n",
        "        score_percent = 90\n",
        "    elif acc >= 0.80:\n",
        "        score_percent = 80\n",
        "    elif acc >= 0.70:\n",
        "        score_percent = 70\n",
        "    elif acc >= 0.60:\n",
        "        score_percent = 60\n",
        "    elif acc >= 0.50:\n",
        "        score_percent = 50\n",
        "    else:\n",
        "        score_percent = 40\n",
        "    score = max_score * score_percent / 100.0 \n",
        "    return score\n",
        "\n",
        "def rubric_naive_bayes(acc, max_score=25):\n",
        "    score_percent = 0\n",
        "    if acc >= 0.75:\n",
        "        score_percent = 100\n",
        "    elif acc >= 0.65:\n",
        "        score_percent = 90\n",
        "    elif acc >= 0.55:\n",
        "        score_percent = 80\n",
        "    elif acc >= 0.40:\n",
        "        score_percent = 70\n",
        "    elif acc >= 0.30:\n",
        "        score_percent = 60\n",
        "    elif acc >= 0.20:\n",
        "        score_percent = 50\n",
        "    elif acc >= 0.10:\n",
        "        score_percent = 45\n",
        "    else:\n",
        "        score_percent = 40\n",
        "    score = max_score * score_percent / 100.0 \n",
        "    return score\n",
        "   \n",
        "def rubric_logistic_regression(acc, max_score=30):\n",
        "    score_percent = 0\n",
        "    if acc >= 0.85:\n",
        "        score_percent = 100\n",
        "    elif acc >= 0.80:\n",
        "        score_percent = 90\n",
        "    elif acc >= 0.75:\n",
        "        score_percent = 80\n",
        "    elif acc >= 0.70:\n",
        "        score_percent = 70\n",
        "    elif acc >= 0.60:\n",
        "        score_percent = 60\n",
        "    elif acc >= 0.50:\n",
        "        score_percent = 55\n",
        "    elif acc >= 0.40:\n",
        "        score_percent = 50\n",
        "    elif acc >= 0.30:\n",
        "        score_percent = 45\n",
        "    else:\n",
        "        score_percent = 40\n",
        "    score = max_score * score_percent / 100.0 \n",
        "    return score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test your code\n",
        "Run the following cell to test your code (or for **debugging**).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Building model Perceptron\n",
            "Skipping download. File already exists: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/train.zip\n",
            "\n",
            "Unzipping: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/train.zip\n",
            "\n",
            "Skipping download. File already exists: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/val.zip\n",
            "\n",
            "Unzipping: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/val.zip\n",
            "\n",
            "Loading dataset with Pandas...\n",
            "Done!\n",
            "Returning data as NumPy array...\n",
            "Training Perceptron...\n",
            "\tUsing hyperparameters: \n",
            "\t\talpha = 1.0\n",
            "\t\tepochs = 2000\n",
            "\t\tseed = 42\n",
            "\t\tTrain acc: 0.53\n",
            "Evaluating Perceptron...\n",
            "\t\tValidation acc: 0.53\n",
            "Elapsed time: 2.1317 seconds\n",
            "\n",
            "Points Earned: 12.5\n",
            "==================================================\n",
            "Building model NaiveBayes\n",
            "Skipping download. File already exists: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/train.zip\n",
            "\n",
            "Unzipping: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/train.zip\n",
            "\n",
            "Skipping download. File already exists: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/val.zip\n",
            "\n",
            "Unzipping: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/val.zip\n",
            "\n",
            "Loading dataset with Pandas...\n",
            "Done!\n",
            "Returning data as NumPy array...\n",
            "Training NaiveBayes...\n",
            "\tUsing hyperparameters: \n",
            "\t\tsmoothing = 0.005\n",
            "\t\tTrain acc: 0.58\n",
            "Evaluating NaiveBayes...\n",
            "\t\tValidation acc: 0.578\n",
            "Elapsed time: 4.4010 seconds\n",
            "\n",
            "Points Earned: 20.0\n",
            "==================================================\n",
            "Building model LogisticRegression\n",
            "Skipping download. File already exists: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/train.zip\n",
            "\n",
            "Unzipping: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/train.zip\n",
            "\n",
            "Skipping download. File already exists: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/val.zip\n",
            "\n",
            "Unzipping: /Users/som/Downloads/Classification Mini Project/datasets/data/MNIST/val.zip\n",
            "\n",
            "Loading dataset with Pandas...\n",
            "Done!\n",
            "Returning data as NumPy array...\n",
            "Training LogisticRegression...\n",
            "\tUsing hyperparameters: \n",
            "\t\talpha = 0.1\n",
            "\t\tepochs = 300\n",
            "\t\tseed = 42\n",
            "\t\tbatch_size = 256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/yp/wyy5kprd443_5jd6zdmkbp800000gn/T/ipykernel_23684/1187489158.py:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  y_one_hot[i, int(y[i])] = 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tTrain acc: 0.925\n",
            "Evaluating LogisticRegression...\n",
            "\t\tValidation acc: 0.915\n",
            "Elapsed time: 62.2804 seconds\n",
            "\n",
            "Points Earned: 30.0\n",
            "==================================================\n",
            "\n",
            "Elapsed time: 68.8132 seconds\n",
            "Tests passed: 3/3, Total Points: 62/80\n",
            "\n",
            "Average Train Accuracy: 0.6784\n",
            "Average Validation Accuracy: 0.6744\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    run_eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
